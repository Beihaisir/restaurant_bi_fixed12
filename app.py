from __future__ import annotations

import io
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import streamlit as st
import altair as alt

from utils.io import read_any_table, read_raw_noheader
from utils.rules import load_rules_xlsx, match_categories, Rule
from utils.transform import build_fact_tables

st.set_page_config(page_title="æ­£å¤§é¤é¥®ç»è¥åˆ†æç³»ç»Ÿ_ä¸´æ—¶", layout="wide")


@dataclass
class StoreBundle:
    store_id: str
    daily: Optional[pd.DataFrame]
    dish: Optional[pd.DataFrame]
    pay: Optional[pd.DataFrame]


DAILY_MUST = ["é—¨åº—ä»£ç ", "é—¨åº—åç§°", "æ—¥æœŸ"]
DISH_MUST = ["åˆ›å»ºæ—¶é—´", "èœå“åç§°", "POSé”€å”®å•å·"]
PAY_MUST = ["POSé”€å”®å•å·", "æ”¯ä»˜ç±»å‹", "æ€»é‡‘é¢"]


def _as_bytes(uploaded) -> bytes:
    return uploaded.getvalue()


def _norm_colset(cols) -> set:
    out = set()
    for c in cols:
        s = str(c).strip().replace(" ", "").replace("\u3000", "")
        out.add(s)
    return out


def detect_table_kind(file_bytes: bytes, filename: str) -> Tuple[str, Optional[str]]:
    raw = read_raw_noheader(file_bytes, filename)

    store_id = None
    for r in range(min(20, len(raw))):
        for cell in raw.iloc[r].astype(str).tolist():
            if "å¯¼å‡ºäºº" in str(cell):
                import re

                m = re.search(r"å¯¼å‡ºäºº[:ï¼š]\s*(\d+)", str(cell))
                if m:
                    store_id = m.group(1)
                    break
        if store_id:
            break

    top_text = " ".join(raw.head(5).astype(str).fillna("").values.flatten().tolist())
    if "æ—¥é”€å”®æŠ¥è¡¨" in top_text:
        return "daily", store_id

    df_dish, _ = read_any_table(file_bytes, filename, DISH_MUST)
    if {"POSé”€å”®å•å·", "èœå“åç§°", "åˆ›å»ºæ—¶é—´"}.issubset(_norm_colset(df_dish.columns)):
        return "dish", store_id

    df_pay, _ = read_any_table(file_bytes, filename, PAY_MUST)
    if {"POSé”€å”®å•å·", "æ”¯ä»˜ç±»å‹"}.issubset(_norm_colset(df_pay.columns)):
        return "pay", store_id

    df_daily, _ = read_any_table(file_bytes, filename, DAILY_MUST)
    cols_daily = _norm_colset(df_daily.columns)
    if ("å«ç¨é”€å”®é¢" in cols_daily) or ("å®¢æµé‡" in cols_daily) or ({"é—¨åº—ä»£ç ", "é—¨åº—åç§°", "æ—¥æœŸ"}.issubset(cols_daily)):
        return "daily", store_id

    return "unknown", store_id


@st.cache_data(show_spinner=False)
def parse_uploaded(files: List, rule_file) -> Tuple[List[StoreBundle], List[str], List[Rule]]:
    rules: List[Rule] = []
    if rule_file is not None:
        rules = load_rules_xlsx(io.BytesIO(_as_bytes(rule_file)))

    bundles: Dict[str, StoreBundle] = {}
    warnings: List[str] = []

    def upsert(store_id: str) -> StoreBundle:
        if store_id not in bundles:
            bundles[store_id] = StoreBundle(store_id=store_id, daily=None, dish=None, pay=None)
        return bundles[store_id]

    for f in files:
        b = _as_bytes(f)
        name = f.name
        kind, store_id = detect_table_kind(b, name)
        if store_id is None:
            store_id = "UNKNOWN"

        if kind == "daily":
            df, _ = read_any_table(b, name, DAILY_MUST)
            upsert(store_id).daily = df
        elif kind == "dish":
            df, _ = read_any_table(b, name, DISH_MUST)
            upsert(store_id).dish = df
        elif kind == "pay":
            df, _ = read_any_table(b, name, PAY_MUST)
            upsert(store_id).pay = df
        else:
            warnings.append(f"æ— æ³•è¯†åˆ«æ–‡ä»¶ç±»å‹ï¼š{name}ï¼ˆå·²è·³è¿‡ï¼‰")

    out = list(bundles.values())
    out.sort(key=lambda x: x.store_id)
    return out, warnings, rules


def fmt_money(x: float) -> str:
    try:
        return f"Â¥{x:,.2f}"
    except Exception:
        return "â€”"

def _safe_div(a: float, b: float) -> float:
    return float(a) / float(b) if b not in (0, 0.0, None) else 0.0


def _dist_from_group(df: pd.DataFrame, key: str, value: str, topn: int = 20) -> Tuple[List[str], np.ndarray]:
    if df is None or df.empty or key not in df.columns or value not in df.columns:
        return [], np.array([], dtype=float)
    g = df.groupby(key, as_index=False)[value].sum().sort_values(value, ascending=False)
    if topn and topn > 0:
        g = g.head(topn)
    return g[key].astype(str).tolist(), g[value].astype(float).to_numpy()


def _align_two(keys_a: List[str], vals_a: np.ndarray, keys_b: List[str], vals_b: np.ndarray) -> Tuple[List[str], np.ndarray, np.ndarray]:
    keys = list(dict.fromkeys(list(keys_a) + list(keys_b)))
    ma = {k: float(v) for k, v in zip(keys_a, vals_a)}
    mb = {k: float(v) for k, v in zip(keys_b, vals_b)}
    a = np.array([ma.get(k, 0.0) for k in keys], dtype=float)
    b = np.array([mb.get(k, 0.0) for k in keys], dtype=float)
    return keys, a, b


def _entropy_share(v: np.ndarray) -> float:
    v = np.asarray(v, dtype=float)
    s = float(np.sum(v))
    if s <= 0:
        return 0.0
    p = v / s
    p = np.where(p <= 0, 1e-12, p)
    h = float(-np.sum(p * np.log(p)))
    hmax = float(np.log(len(p))) if len(p) > 1 else 1.0
    return h / hmax if hmax > 0 else 0.0


def _max_share(v: np.ndarray) -> float:
    s = float(np.sum(v))
    if s <= 0:
        return 0.0
    return float(np.max(v / s))


def _compute_profile_scores(filtered_store: Dict[str, pd.DataFrame]) -> Dict[str, float]:
    o = filtered_store["orders"]
    p = filtered_store["pay"]
    m = filtered_store["items_main"]
    a = filtered_store["items_add"]

    orders = int(o["POSé”€å”®å•å·"].nunique()) if o is not None and not o.empty else 0
    net = float(o["net_amount"].sum()) if o is not None and not o.empty else 0.0
    paid = float(p["æ€»é‡‘é¢"].sum()) if p is not None and not p.empty else 0.0
    aov = _safe_div(net, orders)
    refund_rate = float(o["has_refund"].mean()) if (o is not None and not o.empty and "has_refund" in o.columns) else 0.0
    diff_abs_rate = _safe_div(abs(net - paid), max(net, 1e-9))

    add_orders = int(a["order_id"].nunique()) if a is not None and not a.empty else 0
    add_rate = _safe_div(add_orders, orders)
    add_amt = float(a["amount"].sum()) if a is not None and not a.empty else 0.0
    add_amt_share = _safe_div(add_amt, max(net, 1e-9))

    spec_df = m[m["spec_norm"].notna()].copy() if (m is not None and not m.empty and "spec_norm" in m.columns) else pd.DataFrame()
    _, spec_vals = _dist_from_group(spec_df, "spec_norm", "èœå“æ•°é‡", topn=10)
    spec_entropy = _entropy_share(spec_vals) if len(spec_vals) else 0.0

    _, pay_vals = _dist_from_group(p, "æ”¯ä»˜ç±»å‹", "æ€»é‡‘é¢", topn=10) if (p is not None and not p.empty) else ([], np.array([], dtype=float))
    pay_entropy = _entropy_share(pay_vals) if len(pay_vals) else 0.0
    pay_max_share = _max_share(pay_vals) if len(pay_vals) else 0.0

    if m is not None and not m.empty and "categories" in m.columns:
        ex = m.copy().explode("categories")
        ex["categories"] = ex["categories"].fillna("æœªåˆ†ç±»")
        cat_keys, cat_vals = _dist_from_group(ex, "categories", "ä¼˜æƒ åå°è®¡ä»·æ ¼", topn=50)
        if len(cat_vals):
            g = pd.DataFrame({"k": cat_keys, "v": cat_vals}).sort_values("v", ascending=False)
            top5 = float(g.head(5)["v"].sum())
            cat_top5_share = _safe_div(top5, float(g["v"].sum()))
        else:
            cat_top5_share = 0.0
    else:
        cat_top5_share = 0.0

    if o is not None and not o.empty:
        tmp = o.copy()
        tmp["slot"] = tmp["order_time"].dt.floor("30min").dt.strftime("%H:%M")
        s = tmp.groupby("slot", as_index=False).agg(v=("POSé”€å”®å•å·", "nunique")).sort_values("v", ascending=False)
        top3 = float(s.head(3)["v"].sum())
        peak3_share = _safe_div(top3, float(s["v"].sum()))
    else:
        peak3_share = 0.0

    return {
        "è®¢å•æ•°": orders,
        "åº”æ”¶": net,
        "å®æ”¶": paid,
        "å®¢å•": aov,
        "é€€æ¬¾ç‡": refund_rate,
        "å¯¹è´¦å·®å¼‚ç‡": diff_abs_rate,
        "å•åŠ æ¸—é€ç‡": add_rate,
        "å•åŠ é‡‘é¢å æ¯”": add_amt_share,
        "è§„æ ¼å¤šæ ·æ€§": spec_entropy,
        "æ¸ é“å¤šæ ·æ€§": pay_entropy,
        "æ¸ é“æœ€å¤§å æ¯”": pay_max_share,
        "å“ç±»Top5å æ¯”": cat_top5_share,
        "å³°å€¼Top3å æ¯”": peak3_share,
    }


def _radar_df(scores: Dict[str, float], dims: List[str], normalize_max: Dict[str, float], invert: set) -> pd.DataFrame:
    rows = []
    for d in dims:
        v = float(scores.get(d, 0.0))
        mx = float(normalize_max.get(d, 1.0)) if normalize_max.get(d, 1.0) else 1.0
        x = v / mx if mx > 0 else 0.0
        x = max(0.0, min(1.0, x))
        if d in invert:
            x = 1.0 - x
        rows.append({"ç»´åº¦": d, "å¾—åˆ†": x})
    return pd.DataFrame(rows)


def _radar_chart(df: pd.DataFrame, store_col: str = "é—¨åº—") -> alt.Chart:
    """Radar chart (matplotlib-free) with strong visibility in Streamlit/Altair v6."""
    dims = df["ç»´åº¦"].unique().tolist()
    n = len(dims)
    if n == 0:
        return alt.Chart(pd.DataFrame({"x": [0], "y": [0]})).mark_point().encode(x="x", y="y")

    angle_map = {d: i * 2 * np.pi / n for i, d in enumerate(dims)}
    d2 = df.copy()
    d2["angle"] = d2["ç»´åº¦"].map(angle_map).astype(float)

    # Ensure score is finite
    d2["å¾—åˆ†"] = pd.to_numeric(d2["å¾—åˆ†"], errors="coerce").fillna(0.0).clip(0.0, 1.0)

    d2["x"] = d2["å¾—åˆ†"] * np.cos(d2["angle"])
    d2["y"] = d2["å¾—åˆ†"] * np.sin(d2["angle"])

    # Close polygon per store
    closed = []
    for s, g in d2.groupby(store_col):
        g = g.sort_values("angle")
        g2 = pd.concat([g, g.iloc[[0]]], ignore_index=True)
        closed.append(g2)
    d2c = pd.concat(closed, ignore_index=True)

    # Fixed domain so it never auto-zooms to zero
    enc = dict(
        x=alt.X("x:Q", axis=None, scale=alt.Scale(domain=[-1.15, 1.15])),
        y=alt.Y("y:Q", axis=None, scale=alt.Scale(domain=[-1.15, 1.15])),
        color=alt.Color(f"{store_col}:N", legend=alt.Legend(title="é—¨åº—")),
        tooltip=[store_col, "ç»´åº¦", alt.Tooltip("å¾—åˆ†:Q", format=".2f")],
    )

    base = alt.Chart(d2c).encode(**enc)

    # Grid circles (0.25/0.5/0.75/1.0)
    rings = pd.DataFrame({"r": [0.25, 0.5, 0.75, 1.0]})
    theta = np.linspace(0, 2 * np.pi, 241)
    ring_rows = []
    for r in rings["r"]:
        for t in theta:
            ring_rows.append({"r": float(r), "x": float(r * np.cos(t)), "y": float(r * np.sin(t))})
    ring_df = pd.DataFrame(ring_rows)
    ring = alt.Chart(ring_df).mark_line(opacity=0.18, strokeWidth=2).encode(
        x=alt.X("x:Q", axis=None, scale=alt.Scale(domain=[-1.15, 1.15])),
        y=alt.Y("y:Q", axis=None, scale=alt.Scale(domain=[-1.15, 1.15])),
        detail="r:N",
    )

    # Axes lines + labels
    axes_rows = []
    for d, ang in angle_map.items():
        axes_rows.append({"ç»´åº¦": d, "x": 0.0, "y": 0.0, "x2": float(np.cos(ang)), "y2": float(np.sin(ang))})
    axes_df = pd.DataFrame(axes_rows)
    axes = alt.Chart(axes_df).mark_rule(opacity=0.30, strokeWidth=2).encode(
        x=alt.X("x:Q", axis=None, scale=alt.Scale(domain=[-1.15, 1.15])),
        y=alt.Y("y:Q", axis=None, scale=alt.Scale(domain=[-1.15, 1.15])),
        x2="x2:Q",
        y2="y2:Q",
    )
    labels = alt.Chart(axes_df).mark_text(align="left", dx=8, dy=8, fontSize=12).encode(
        x=alt.X("x2:Q", axis=None, scale=alt.Scale(domain=[-1.15, 1.15])),
        y=alt.Y("y2:Q", axis=None, scale=alt.Scale(domain=[-1.15, 1.15])),
        text="ç»´åº¦:N",
    )

    # Stronger polygon + points
    poly = base.mark_line(strokeWidth=4).encode(order=alt.Order("angle:Q"))
    pts = base.mark_point(filled=True, size=140, opacity=0.95)

    return (ring + axes + poly + pts + labels).properties(height=460).configure_view(stroke=None)



import uuid

def _dl_key(tag: str, sid: str | None = None) -> str:
    """
    Generate a runtime-unique key for Streamlit elements (especially inside loops).
    Using uuid avoids DuplicateElementKey across reruns and repeated blocks.
    """
    if sid is None:
        return f"dl_{tag}_{uuid.uuid4().hex}"
    return f"dl_{tag}_{sid}_{uuid.uuid4().hex}"


def halfhour_options(min_dt: pd.Timestamp, max_dt: pd.Timestamp) -> List[pd.Timestamp]:
    if pd.isna(min_dt) or pd.isna(max_dt):
        return []
    start = min_dt.floor("30min")
    end = max_dt.ceil("30min")
    return list(pd.date_range(start, end, freq="30min"))


def apply_time_filter(df: pd.DataFrame, col: str, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    x = df.copy()
    if col not in x.columns:
        return x
    x[col] = pd.to_datetime(x[col], errors="coerce")
    return x[(x[col] >= start) & (x[col] <= end)].copy()


def _base_items(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    if "ç±»å‹_norm" in df.columns:
        return df[df["ç±»å‹_norm"].isin(["èœå“", "å¥—é¤"])].copy()
    return df[df["ç±»å‹"].astype(str).str.contains("èœå“|å¥—é¤", na=False)].copy()


def _share_table(df_long: pd.DataFrame, store_col: str, key_col: str, val_col: str, topn: int) -> pd.DataFrame:
    tot = df_long.groupby(key_col, as_index=False)[val_col].sum().sort_values(val_col, ascending=False).head(topn)
    keys = tot[key_col].tolist()
    sub = df_long[df_long[key_col].isin(keys)].copy()
    denom = sub.groupby(store_col, as_index=False)[val_col].sum().rename(columns={val_col: "_den"})
    sub = sub.merge(denom, on=store_col, how="left")
    sub["share"] = sub[val_col] / sub["_den"].replace(0, np.nan)
    out = sub.pivot_table(index=key_col, columns=store_col, values="share", aggfunc="sum").fillna(0.0)
    out = out.loc[keys]
    return out


def _js_divergence(p: np.ndarray, q: np.ndarray) -> float:
    """
    Jensenâ€“Shannon divergence between two non-negative vectors.
    Returns a non-negative float (0 means identical distributions).
    """
    p = np.asarray(p, dtype=float)
    q = np.asarray(q, dtype=float)

    # Handle empty/zero vectors robustly
    ps = float(np.nansum(p))
    qs = float(np.nansum(q))
    if ps <= 0 and qs <= 0:
        return 0.0
    if ps <= 0:
        p = np.zeros_like(q, dtype=float)
        ps = 1.0
    if qs <= 0:
        q = np.zeros_like(p, dtype=float)
        qs = 1.0

    p = np.where(np.isfinite(p), p, 0.0) / ps
    q = np.where(np.isfinite(q), q, 0.0) / qs
    m = 0.5 * (p + q)

    def _kl(x: np.ndarray, y: np.ndarray) -> float:
        x = np.where(x <= 0, 1e-12, x)
        y = np.where(y <= 0, 1e-12, y)
        return float(np.sum(x * np.log(x / y)))

    return 0.5 * _kl(p, m) + 0.5 * _kl(q, m)

def _simple_gradient_styler(df: pd.DataFrame):
    """
    Matplotlib-free gradient styler for Streamlit Cloud.
    pandas Styler.background_gradient requires matplotlib; this does not.
    """
    if df is None or df.empty:
        return df

    vals = df.to_numpy(dtype=float, copy=True)
    finite = np.isfinite(vals)
    if not finite.any():
        return df

    vmin = float(np.nanmin(vals[finite]))
    vmax = float(np.nanmax(vals[finite]))
    if vmin == vmax:
        vmax = vmin + 1.0

    def _color(v):
        if v is None:
            return ""
        try:
            fv = float(v)
        except Exception:
            return ""
        if np.isnan(fv) or np.isinf(fv):
            return ""
        x = (fv - vmin) / (vmax - vmin)
        x = max(0.0, min(1.0, x))
        # light -> dark blue
        r1, g1, b1 = (246, 248, 255)
        r2, g2, b2 = (30, 98, 211)
        r = int(r1 + (r2 - r1) * x)
        g = int(g1 + (g2 - g1) * x)
        b = int(b1 + (b2 - b1) * x)
        txt = "#ffffff" if x > 0.55 else "#111111"
        return f"background-color: rgb({r},{g},{b}); color: {txt};"

    return df.style.applymap(_color)

    def _kl(x, y):
        x = np.where(x <= 0, 1e-12, x)
        y = np.where(y <= 0, 1e-12, y)
        return float(np.sum(x * np.log(x / y)))

    return 0.5 * _kl(p, m) + 0.5 * _kl(q, m)


def main() -> None:
    st.title("ğŸ½ï¸ æ­£å¤§é¤é¥®ç»è¥åˆ†æç³»ç»Ÿ_ä¸´æ—¶ç‰ˆ")

    with st.sidebar:
        st.header("æ•°æ®è¾“å…¥")
        rule_file = st.file_uploader("ä¸Šä¼ ï¼šåˆ†ç±»è§„åˆ™æ¨¡æ¿ï¼ˆxlsxï¼ŒSheet=è§„åˆ™è¡¨ï¼‰", type=["xlsx"], accept_multiple_files=False)
        files = st.file_uploader(
            "ä¸Šä¼ ï¼šä¸‰ç±»æŠ¥è¡¨ï¼ˆå¯å¤šé—¨åº—ã€å¤šæ–‡ä»¶ï¼›æ”¯æŒ xls/xlsx/csvï¼‰",
            type=["xls", "xlsx", "csv"],
            accept_multiple_files=True,
        )
        st.caption("å£å¾„ï¼šæ—¶é—´æœ€å°30åˆ†é’Ÿï¼›â€œåŠ xxâ€ä¸ºå•åŠ ï¼ˆåŠ å¤šå®é™¤å¤–ï¼‰ï¼›å¤©éº»é¢å¹¶å…¥ç»†é¢ï¼›â€œæ ‡å‡†â€ä»…ç»Ÿè®¡ä¸ºã€å¥—é¤ã€‘çš„æ ‡å‡†è¡Œã€‚")

    if not files:
        st.info("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æŠ¥è¡¨æ–‡ä»¶ã€‚")
        return

    bundles, warnings, rules = parse_uploaded(files, rule_file)

    if warnings:
        with st.expander("âš ï¸ æ–‡ä»¶è¯†åˆ«è­¦å‘Š", expanded=False):
            for w in warnings:
                st.warning(w)

    analyzable = [b for b in bundles if b.dish is not None and b.pay is not None and b.daily is not None and b.store_id != "UNKNOWN"]
    missing = [b for b in bundles if b not in analyzable]

    if missing:
        with st.expander("âš ï¸ ç¼ºè¡¨é—¨åº—ï¼ˆä¸è¿›å…¥åˆ†æï¼‰", expanded=False):
            st.dataframe(
                pd.DataFrame(
                    [{"store_id": b.store_id, "æœ‰æ—¥é”€å”®": b.daily is not None, "æœ‰èœå“æ˜ç»†": b.dish is not None, "æœ‰æ”¯ä»˜æ˜ç»†": b.pay is not None} for b in missing]
                ),
                use_container_width=True,
            )

    if not analyzable:
        st.error("æ²¡æœ‰â€œä¸‰è¡¨é½å…¨â€çš„é—¨åº—ï¼Œæ— æ³•åˆ†æã€‚")
        return

    store_ids = [b.store_id for b in analyzable]
    sel_stores = st.multiselect("é€‰æ‹©é—¨åº—ï¼ˆæ”¯æŒå¤šåº—å¯¹æ¯”ï¼‰", options=store_ids, default=store_ids[:1])
    if not sel_stores:
        st.stop()

    facts_by_store: Dict[str, Dict[str, pd.DataFrame]] = {}
    for b in analyzable:
        if b.store_id in sel_stores:
            facts_by_store[b.store_id] = build_fact_tables(b.dish, b.pay, rules, b.store_id)

    all_orders = pd.concat([facts_by_store[s]["fact_orders"] for s in sel_stores], ignore_index=True)
    min_dt = all_orders["order_time"].min()
    max_dt = all_orders["order_time"].max()
    opts = halfhour_options(min_dt, max_dt)
    if not opts:
        st.error("æ— æ³•ä»æ•°æ®ä¸­è§£æåˆ›å»ºæ—¶é—´ã€‚")
        return

    c1, c2 = st.columns(2)
    with c1:
        start = st.selectbox("å¼€å§‹æ—¶é—´ï¼ˆ30åˆ†é’Ÿç²’åº¦ï¼‰", options=opts, index=0, format_func=lambda x: x.strftime("%Y-%m-%d %H:%M"))
    with c2:
        end = st.selectbox("ç»“æŸæ—¶é—´ï¼ˆ30åˆ†é’Ÿç²’åº¦ï¼‰", options=opts, index=len(opts) - 1, format_func=lambda x: x.strftime("%Y-%m-%d %H:%M"))

    if start > end:
        st.error("å¼€å§‹æ—¶é—´ä¸èƒ½æ™šäºç»“æŸæ—¶é—´ã€‚")
        return

    filtered: Dict[str, Dict[str, pd.DataFrame]] = {}
    for sid in sel_stores:
        f = facts_by_store[sid]
        filtered[sid] = {
            "items_main": apply_time_filter(f["fact_items_main"], "åˆ›å»ºæ—¶é—´", start, end),
            "items_add": apply_time_filter(f["fact_items_add"], "created_at", start, end),
            "pay": apply_time_filter(f["fact_pay"], "order_time", start, end),
            "orders": apply_time_filter(f["fact_orders"], "order_time", start, end),
        }

    tabs = st.tabs(
        [
            "â‘  è‘£äº‹/è‚¡ä¸œæ€»è§ˆ",
            "â‘¡ é—¨åº—å¯¹æ¯”",
            "â‘¢ è§„æ ¼",
            "â‘£ å“ç±»ç»“æ„",
            "â‘¤ å•åŠ åˆ†æ",
            "â‘¥ æ”¯ä»˜æ¸ é“",
            "â‘¦ é€€æ¬¾/å¼‚å¸¸ä¸å¯¹è´¦",
            "â‘§ æœªåˆ†ç±»æ± ï¼ˆå¯å¯¼å‡ºï¼‰",
            "â‘¨ æ˜ç»†å¯¼å‡º",
            "â‘© æ—¶æ®µçƒ­åŠ›å›¾",
            "â‘ª é—¨åº—ç”»åƒå¡ï¼ˆä¸¤åº—å¯¹æ¯”ï¼‰",
        ]
    )

    # â‘  æ€»è§ˆ
    with tabs[0]:
        st.subheader("è‘£äº‹/è‚¡ä¸œè§†è§’ï¼šè§„æ¨¡ã€æ•ˆç‡ã€ç»“æ„ã€é£é™©")

        rows = []
        for sid in sel_stores:
            o = filtered[sid]["orders"]
            p = filtered[sid]["pay"]
            orders = int(o["POSé”€å”®å•å·"].nunique()) if not o.empty else 0
            rows.append(
                {
                    "store_id": sid,
                    "è®¢å•æ•°": orders,
                    "èœå“é”€é‡": float(o["dish_qty"].sum()) if not o.empty else 0.0,
                    "èœå“åº”æ”¶(ä¼˜æƒ å)": float(o["net_amount"].sum()) if not o.empty else 0.0,
                    "æ”¯ä»˜å®æ”¶": float(p["æ€»é‡‘é¢"].sum()) if not p.empty else 0.0,
                    "é€€æ¬¾å•å æ¯”": float(o["has_refund"].mean()) if not o.empty else 0.0,
                    "å®¢å•(åº”æ”¶/è®¢å•)": (float(o["net_amount"].sum()) / orders) if orders else np.nan,
                }
            )
        dfk = pd.DataFrame(rows)
        k1, k2, k3, k4 = st.columns(4)
        k1.metric("é€‰ä¸­é—¨åº—è®¢å•æ•°", int(dfk["è®¢å•æ•°"].sum()))
        k2.metric("é€‰ä¸­é—¨åº—èœå“é”€é‡", f"{dfk['èœå“é”€é‡'].sum():,.0f}")
        k3.metric("é€‰ä¸­é—¨åº—èœå“åº”æ”¶(ä¼˜æƒ å)", fmt_money(dfk["èœå“åº”æ”¶(ä¼˜æƒ å)"].sum()))
        k4.metric("é€‰ä¸­é—¨åº—æ”¯ä»˜å®æ”¶", fmt_money(dfk["æ”¯ä»˜å®æ”¶"].sum()))
        st.dataframe(dfk, use_container_width=True)

        oall = pd.concat([filtered[s]["orders"] for s in sel_stores], ignore_index=True)
        if not oall.empty:
            oall["bucket"] = oall["order_time"].dt.floor("30min")
            trend = oall.groupby("bucket", as_index=False).agg(è®¢å•æ•°=("POSé”€å”®å•å·", "nunique"), èœå“åº”æ”¶=("net_amount", "sum")).sort_values("bucket")
            st.line_chart(trend.set_index("bucket")[["è®¢å•æ•°", "èœå“åº”æ”¶"]])
            st.markdown("**å³°å€¼æ—¶æ®µ Top10ï¼ˆæŒ‰è®¢å•æ•°ï¼‰**")
            st.dataframe(trend.sort_values("è®¢å•æ•°", ascending=False).head(10), use_container_width=True)

        main_all = pd.concat([filtered[s]["items_main"] for s in sel_stores], ignore_index=True)
        base_items = _base_items(main_all)
        if not base_items.empty:
            top_rev = (
                base_items.groupby("èœå“åç§°", as_index=False)
                .agg(åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"), é”€é‡=("èœå“æ•°é‡", "sum"), è®¢å•æ•°=("POSé”€å”®å•å·", "nunique"))
                .sort_values(["åº”æ”¶", "é”€é‡"], ascending=False)
                .head(20)
            )
            st.markdown("### Top20 èœå“ï¼ˆæŒ‰åº”æ”¶æ’åºï¼‰")
            st.dataframe(top_rev, use_container_width=True)
            st.bar_chart(top_rev.set_index("èœå“åç§°")[["åº”æ”¶"]])

            dish_rev = base_items.groupby("èœå“åç§°", as_index=False).agg(åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum")).sort_values("åº”æ”¶", ascending=False)
            dish_rev["ç´¯è®¡åº”æ”¶"] = dish_rev["åº”æ”¶"].cumsum()
            total_rev = dish_rev["åº”æ”¶"].sum()
            dish_rev["ç´¯è®¡å æ¯”"] = dish_rev["ç´¯è®¡åº”æ”¶"] / total_rev if total_rev else 0
            n80 = int((dish_rev["ç´¯è®¡å æ¯”"] <= 0.8).sum() + 1) if total_rev else 0
            st.markdown("### çˆ†å“/é•¿å°¾ï¼ˆå¸•ç´¯æ‰˜ï¼‰")
            st.write(f"è¾¾åˆ° **80%åº”æ”¶** éœ€è¦çš„èœå“æ•°ï¼š**{n80}** / æ€»èœå“æ•° {len(dish_rev)}")
            st.dataframe(dish_rev.head(50), use_container_width=True)

        add_all = pd.concat([filtered[s]["items_add"] for s in sel_stores], ignore_index=True)
        if not add_all.empty:
            top_add = add_all.groupby("add_display", as_index=False).agg(å•åŠ é‡‘é¢=("amount", "sum"), é”€é‡=("qty", "sum"), è®¢å•æ•°=("order_id", "nunique")).sort_values(["å•åŠ é‡‘é¢", "é”€é‡"], ascending=False).head(20)
            st.markdown("### Top20 å•åŠ ï¼ˆæŒ‰å•åŠ é‡‘é¢æ’åºï¼‰")
            st.dataframe(top_add, use_container_width=True)
            st.bar_chart(top_add.set_index("add_display")[["å•åŠ é‡‘é¢"]])

    # â‘¡ é—¨åº—å¯¹æ¯”ï¼ˆç»“æ„å¯¹æ¯” + åç¦»åº¦ï¼šä¿®å¤sel_storesä½œç”¨åŸŸï¼‰
    with tabs[1]:
        st.subheader("é—¨åº—å¯¹æ¯”ï¼šåŒå£å¾„çœ‹å·®å¼‚ï¼ˆåº—é•¿/åŒºåŸŸç»ç†/æ€»éƒ¨ï¼‰")
        rows = []
        for sid in sel_stores:
            o = filtered[sid]["orders"]
            p = filtered[sid]["pay"]
            orders = int(o["POSé”€å”®å•å·"].nunique()) if not o.empty else 0
            net = float(o["net_amount"].sum()) if not o.empty else 0.0
            paid = float(p["æ€»é‡‘é¢"].sum()) if not p.empty else 0.0
            rows.append({"store_id": sid, "è®¢å•æ•°": orders, "åº”æ”¶(ä¼˜æƒ å)": net, "å®æ”¶": paid, "åº”æ”¶-å®æ”¶å·®å¼‚": net - paid, "å®¢å•(åº”æ”¶/è®¢å•)": (net / orders) if orders else np.nan})
        df = pd.DataFrame(rows).sort_values("åº”æ”¶(ä¼˜æƒ å)", ascending=False)
        st.dataframe(df, use_container_width=True)
        st.bar_chart(df.set_index("store_id")[["åº”æ”¶(ä¼˜æƒ å)", "å®æ”¶"]])
        c1, c2 = st.columns(2)
        with c1:
            st.bar_chart(df.set_index("store_id")[["åº”æ”¶-å®æ”¶å·®å¼‚"]])
        with c2:
            st.bar_chart(df.set_index("store_id")[["å®¢å•(åº”æ”¶/è®¢å•)"]])

        st.markdown("### åŒåº—ç»“æ„å¯¹æ¯”ï¼ˆæ€»éƒ¨/åŒºåŸŸï¼šæ‰¾åç¦»ã€æ‰¾å¯å¤åˆ¶æ‰“æ³•ï¼‰")
        dim = st.selectbox("é€‰æ‹©ç»“æ„ç»´åº¦", options=["è§„æ ¼ç»“æ„", "å“ç±»ç»“æ„", "å•åŠ ç»“æ„", "æ”¯ä»˜ç»“æ„"], index=0)
        metric = st.selectbox("é€‰æ‹©æŒ‡æ ‡", options=["åº”æ”¶", "é”€é‡/ç¬”æ•°"], index=0, key="cmp_metric")

        def build_long() -> pd.DataFrame:
            rows2 = []
            if dim == "è§„æ ¼ç»“æ„":
                for sid in sel_stores:
                    m = _base_items(filtered[sid]["items_main"])
                    x = m[m["spec_norm"].notna()].copy() if not m.empty else pd.DataFrame()
                    if x.empty:
                        continue
                    if metric == "åº”æ”¶":
                        g = x.groupby("spec_norm", as_index=False).agg(v=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"))
                    else:
                        g = x.groupby("spec_norm", as_index=False).agg(v=("èœå“æ•°é‡", "sum"))
                    g["store_id"] = sid
                    g = g.rename(columns={"spec_norm": "k"})
                    rows2.append(g)
            elif dim == "å“ç±»ç»“æ„":
                for sid in sel_stores:
                    m = filtered[sid]["items_main"]
                    if m.empty:
                        continue
                    ex = m.copy().explode("categories")
                    ex["categories"] = ex["categories"].fillna("æœªåˆ†ç±»")
                    if metric == "åº”æ”¶":
                        g = ex.groupby("categories", as_index=False).agg(v=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"))
                    else:
                        g = ex.groupby("categories", as_index=False).agg(v=("èœå“æ•°é‡", "sum"))
                    g["store_id"] = sid
                    g = g.rename(columns={"categories": "k"})
                    rows2.append(g)
            elif dim == "å•åŠ ç»“æ„":
                for sid in sel_stores:
                    a = filtered[sid]["items_add"]
                    if a.empty:
                        continue
                    if metric == "åº”æ”¶":
                        g = a.groupby("add_display", as_index=False).agg(v=("amount", "sum"))
                    else:
                        g = a.groupby("add_display", as_index=False).agg(v=("order_id", "nunique"))
                    g["store_id"] = sid
                    g = g.rename(columns={"add_display": "k"})
                    rows2.append(g)
            else:
                for sid in sel_stores:
                    p = filtered[sid]["pay"]
                    if p.empty:
                        continue
                    if metric == "åº”æ”¶":
                        g = p.groupby("æ”¯ä»˜ç±»å‹", as_index=False).agg(v=("æ€»é‡‘é¢", "sum"))
                    else:
                        g = p.groupby("æ”¯ä»˜ç±»å‹", as_index=False).agg(v=("POSé”€å”®å•å·", "count"))
                    g["store_id"] = sid
                    g = g.rename(columns={"æ”¯ä»˜ç±»å‹": "k"})
                    rows2.append(g)

            if rows2:
                return pd.concat(rows2, ignore_index=True)
            return pd.DataFrame(columns=["store_id", "k", "v"])

        long = build_long()
        if long.empty:
            st.info("æš‚æ— æ•°æ®ç”¨äºç»“æ„å¯¹æ¯”ã€‚")
        else:
            topn = 6 if dim == "è§„æ ¼ç»“æ„" else (12 if dim == "å“ç±»ç»“æ„" else 10)
            share = _share_table(long, "store_id", "k", "v", topn=topn)
            st.dataframe(share.style.format("{:.1%}"), use_container_width=True)

            chart = alt.Chart(long).mark_bar().encode(
                x=alt.X("store_id:N", title="é—¨åº—"),
                y=alt.Y("v:Q", title="å€¼"),
                color=alt.Color("k:N", title=dim.replace("ç»“æ„", "")),
                tooltip=["store_id", "k", "v"],
            ).properties(height=420)
            st.altair_chart(chart, use_container_width=True)

            # åç¦»åº¦
            st.markdown("### åç¦»åº¦æ’åï¼šå“ªå®¶é—¨åº—æœ€â€˜ä¸ä¸€æ ·â€™ï¼Ÿå“ªå®¶å¯åšæ ‡æ†ï¼Ÿ")
            mat = share.T  # store x key
            mean = mat.mean(axis=0).values
            rows3 = []
            for sid in mat.index:
                js = _js_divergence(mat.loc[sid].values, mean)
                rows3.append({"store_id": sid, "åç¦»åº¦(JS)": js})
            ddf = pd.DataFrame(rows3).sort_values("åç¦»åº¦(JS)", ascending=False)
            st.dataframe(ddf, use_container_width=True)
            if len(ddf) >= 2:
                bench = ddf.sort_values("åç¦»åº¦(JS)", ascending=True).iloc[0]["store_id"]
                outlier = ddf.iloc[0]["store_id"]
                st.write(f"**å»ºè®®**ï¼šå¯å…ˆæŠŠåç¦»åº¦æœ€ä½çš„é—¨åº— **{bench}** ä½œä¸ºâ€œæ ‡æ†ç»“æ„â€ï¼Œé‡ç‚¹å¤ç›˜åç¦»åº¦æœ€é«˜çš„é—¨åº— **{outlier}** çš„åŸå› ï¼ˆå®¢ç¾¤/æ—¶æ®µ/å¥—é¤å æ¯”/æ¸ é“ï¼‰ã€‚")

    # â‘¢ è§„æ ¼
    with tabs[2]:
        st.subheader("è§„æ ¼ï¼šä¸»é£Ÿç»“æ„ï¼ˆå«â€œæ ‡å‡†â€=å¥—é¤æ ‡å‡†ï¼‰")
        st.caption("è§„æ ¼åˆ†å¸ƒåªç»Ÿè®¡ï¼šæ ‡å‡† / å®½é¢ / ç»†é¢(å«å¤©éº»é¢) / ç±³é¥­ / å®½ç²‰(å«ç²‰) / æ— éœ€ä¸»é£Ÿï¼›â€œæ ‡å‡†â€ä»…æ¥æºäº ç±»å‹=å¥—é¤ çš„æ ‡å‡†è¡Œã€‚")
        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            m = filtered[sid]["items_main"]
            if m.empty:
                st.info("æ— æ•°æ®")
                continue
            base = _base_items(m)
            spec_base = base[base["spec_norm"].notna()].copy()
            if spec_base.empty:
                st.info("è¯¥æ—¶é—´èŒƒå›´å†…æ²¡æœ‰å‘½ä¸­è§„æ ¼ç™½åå•çš„æ•°æ®ã€‚")
                continue
            spec = spec_base.groupby("spec_norm", as_index=False).agg(é”€é‡=("èœå“æ•°é‡", "sum"), åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"), è¡Œæ•°=("èœå“åç§°", "count"), è®¢å•æ•°=("POSé”€å”®å•å·", "nunique")).sort_values(["é”€é‡", "åº”æ”¶"], ascending=False)
            spec["é”€é‡å æ¯”"] = spec["é”€é‡"] / spec["é”€é‡"].sum() if spec["é”€é‡"].sum() else 0
            spec["åº”æ”¶å æ¯”"] = spec["åº”æ”¶"] / spec["åº”æ”¶"].sum() if spec["åº”æ”¶"].sum() else 0
            st.dataframe(spec, use_container_width=True)
            c1, c2 = st.columns(2)
            with c1:
                st.bar_chart(spec.set_index("spec_norm")[["é”€é‡"]])
            with c2:
                st.bar_chart(spec.set_index("spec_norm")[["åº”æ”¶"]])
            spec_base["bucket"] = spec_base["åˆ›å»ºæ—¶é—´"].dt.floor("30min")
            top_specs = spec["spec_norm"].head(5).tolist()
            pivot = spec_base[spec_base["spec_norm"].isin(top_specs)].groupby(["bucket", "spec_norm"], as_index=False).agg(é”€é‡=("èœå“æ•°é‡", "sum"))
            if not pivot.empty:
                piv = pivot.pivot(index="bucket", columns="spec_norm", values="é”€é‡").fillna(0).sort_index()
                st.line_chart(piv)

    # â‘£ å“ç±»ç»“æ„
    with tabs[3]:
        st.subheader("å“ç±»ç»“æ„ï¼šè§„åˆ™æ¨¡æ¿å‘½ä¸­ï¼ˆå¤šæ ‡ç­¾ï¼‰")
        st.caption("ä¸€ä¸ªèœå“å¯å‘½ä¸­å¤šä¸ªåˆ†ç±»ï¼Œå‘½ä¸­å³å„è®¡ä¸€æ¬¡ï¼›æœªå‘½ä¸­è¿›å…¥æœªåˆ†ç±»æ± ã€‚")
        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            m = filtered[sid]["items_main"]
            if m.empty:
                st.info("æ— æ•°æ®")
                continue
            exploded = m.copy().explode("categories")
            exploded["categories"] = exploded["categories"].fillna("æœªåˆ†ç±»")
            cat = exploded.groupby("categories", as_index=False).agg(é”€é‡=("èœå“æ•°é‡", "sum"), åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"), èœå“è¡Œæ•°=("èœå“åç§°", "count")).sort_values("åº”æ”¶", ascending=False)
            st.dataframe(cat, use_container_width=True)
            st.bar_chart(cat.set_index("categories")[["åº”æ”¶"]])
            topn = st.slider(f"TopN èœå“ï¼ˆé—¨åº— {sid}ï¼‰", min_value=5, max_value=50, value=20, step=5, key=f"topn_{sid}")
            cats = ["å…¨éƒ¨"] + sorted(exploded["categories"].dropna().unique().tolist())
            sel_cat = st.selectbox(f"é€‰æ‹©åˆ†ç±»ï¼ˆé—¨åº— {sid}ï¼‰", options=cats, key=f"selcat_{sid}")
            view = exploded if sel_cat == "å…¨éƒ¨" else exploded[exploded["categories"] == sel_cat]
            top_items = view.groupby("èœå“åç§°", as_index=False).agg(åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"), é”€é‡=("èœå“æ•°é‡", "sum"), è®¢å•æ•°=("POSé”€å”®å•å·", "nunique")).sort_values(["åº”æ”¶", "é”€é‡"], ascending=False).head(topn)
            st.dataframe(top_items, use_container_width=True)

    # â‘¤ å•åŠ åˆ†æ
    with tabs[4]:
        st.subheader("å•åŠ åˆ†æï¼šåŠ æ–™å¸¦æ¥çš„ç»“æ„ä¸å®¢å•æå‡ï¼ˆä¸ä¸»èœä¸¥æ ¼éš”ç¦»ï¼‰")
        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            a = filtered[sid]["items_add"]
            if a.empty:
                st.info("æ— å•åŠ è®°å½•")
                continue
            add = a.groupby("add_display", as_index=False).agg(é”€é‡=("qty", "sum"), å•åŠ é‡‘é¢=("amount", "sum"), è®¢å•æ•°=("order_id", "nunique"), æ¥æº=("source", lambda s: ",".join(sorted(set(map(str, s)))))).sort_values(["å•åŠ é‡‘é¢", "é”€é‡"], ascending=False)
            st.dataframe(add, use_container_width=True)
            st.bar_chart(add.set_index("add_display")[["å•åŠ é‡‘é¢"]])
            orders = filtered[sid]["orders"]
            add_orders = int(a["order_id"].nunique())
            total_orders = int(orders["POSé”€å”®å•å·"].nunique()) if not orders.empty else 0
            st.metric("å•åŠ æ¸—é€ç‡ï¼ˆå«å•åŠ è®¢å•/æ€»è®¢å•ï¼‰", f"{(add_orders / total_orders * 100) if total_orders else 0:.1f}%")
            if not orders.empty:
                add_set = set(a["order_id"].dropna().astype(str).tolist())
                o2 = orders.copy()
                o2["has_add"] = o2["POSé”€å”®å•å·"].astype(str).isin(add_set)
                grp = o2.groupby("has_add", as_index=False).agg(è®¢å•æ•°=("POSé”€å”®å•å·", "nunique"), åº”æ”¶=("net_amount", "sum"))
                grp["å®¢å•(åº”æ”¶/è®¢å•)"] = grp["åº”æ”¶"] / grp["è®¢å•æ•°"].replace(0, np.nan)
                st.markdown("**æœ‰å•åŠ  vs æ— å•åŠ ï¼ˆå®¢å•æå‡ï¼‰**")
                st.dataframe(grp, use_container_width=True)

    # â‘¥ æ”¯ä»˜æ¸ é“
    with tabs[5]:
        st.subheader("æ”¯ä»˜æ¸ é“ï¼šæ¸ é“ç»“æ„ã€å›¢è´­æ¸—é€ã€æ··åˆæ”¯ä»˜")
        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            p = filtered[sid]["pay"]
            if p.empty:
                st.warning("æ— æ”¯ä»˜æ•°æ®ï¼ˆè¯¥é—¨åº—åœ¨ç­›é€‰æ—¶é—´èŒƒå›´å†…æ”¯ä»˜è¡¨æœªå…³è”åˆ°ä»»ä½•è®¢å•ï¼Œæˆ–æ”¯ä»˜è¡¨æœªè¢«æ­£ç¡®è¯†åˆ«ï¼‰")
                continue
            pay = p.groupby("æ”¯ä»˜ç±»å‹", as_index=False).agg(å®æ”¶=("æ€»é‡‘é¢", "sum"), æ”¯ä»˜ç¬”æ•°=("POSé”€å”®å•å·", "count"), æ¶‰åŠè®¢å•=("POSé”€å”®å•å·", "nunique")).sort_values(["å®æ”¶", "æ”¯ä»˜ç¬”æ•°"], ascending=False)
            st.dataframe(pay, use_container_width=True)

    # â‘¦ é€€æ¬¾/å¼‚å¸¸ä¸å¯¹è´¦ï¼ˆä¿æŒfixed10åŠŸèƒ½ + å¢å¼ºå·²åœ¨ä¸Šé¢å®ç°ï¼‰
    

        if p is not None and not p.empty:
            pay_kind = p.groupby(["store_id", "POSé”€å”®å•å·"], as_index=False).agg(
                paid=("æ€»é‡‘é¢", "sum"),
                k=("æ”¯ä»˜ç±»å‹", lambda s: "æ··åˆ" if s.nunique() > 1 else str(list(s)[0])),
            )
            rr = o.merge(pay_kind, on=["store_id", "POSé”€å”®å•å·"], how="left")
            rr["paid"] = rr["paid"].fillna(0.0)
            rr["k"] = rr["k"].fillna("æœªçŸ¥")
            rr["diff"] = rr["net_amount"] - rr["paid"]

            byk = rr.groupby("k", as_index=False).agg(
                è®¢å•æ•°=("POSé”€å”®å•å·", "nunique"),
                åº”æ”¶=("net_amount", "sum"),
                å®æ”¶=("paid", "sum"),
                å·®å¼‚=("diff", "sum"),
            ).sort_values(["å·®å¼‚", "è®¢å•æ•°"], ascending=False)
            st.markdown("**æŒ‰æ”¯ä»˜æ¸ é“åˆ†è§£ï¼ˆå·®å¼‚=åº”æ”¶-å®æ”¶ï¼‰**")
            st.dataframe(byk, use_container_width=True)
            st.bar_chart(byk.set_index("k")[["å·®å¼‚"]])

            rr["slot"] = rr["order_time"].dt.floor("30min")
            bys = rr.groupby("slot", as_index=False).agg(
                è®¢å•æ•°=("POSé”€å”®å•å·", "nunique"),
                åº”æ”¶=("net_amount", "sum"),
                å®æ”¶=("paid", "sum"),
                å·®å¼‚=("diff", "sum"),
            ).sort_values("slot")
            st.markdown("**æŒ‰åŠå°æ—¶åˆ†è§£ï¼ˆå·®å¼‚è¶‹åŠ¿ï¼‰**")
            st.line_chart(bys.set_index("slot")[["å·®å¼‚", "åº”æ”¶", "å®æ”¶"]])

        refund_rate = float(o["has_refund"].mean()) if ("has_refund" in o.columns and not o.empty) else 0.0
        st.metric("é€€æ¬¾å•å æ¯”ï¼ˆèœå“è¡¨å­˜åœ¨POSé€€æ¬¾å•å·ï¼‰", f"{refund_rate * 100:.1f}%")

        st.markdown("### é€€æ¬¾å½’å› ï¼šèœå“/æ—¶æ®µ/æ¸ é“/è§„æ ¼/å“ç±»ï¼ˆå¯å¯¼å‡ºï¼‰")
        items = filtered[sid]["items_main"]
        ref_rows = items[items["POSé€€æ¬¾å•å·"].notna()].copy() if (items is not None and not items.empty and "POSé€€æ¬¾å•å·" in items.columns) else pd.DataFrame()
        if ref_rows.empty:
            st.info("è¯¥æ—¶é—´èŒƒå›´å†…æœªè¯†åˆ«åˆ°é€€æ¬¾è¡Œï¼ˆPOSé€€æ¬¾å•å· ä¸ºç©ºï¼‰ã€‚")
        else:
            ref_rows["slot"] = ref_rows["åˆ›å»ºæ—¶é—´"].dt.floor("30min")

            top_dish = ref_rows.groupby("èœå“åç§°", as_index=False).agg(
                é€€æ¬¾è¡Œæ•°=("èœå“åç§°", "count"),
                é€€æ¬¾åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"),
                é€€æ¬¾è®¢å•=("POSé”€å”®å•å·", "nunique"),
            ).sort_values(["é€€æ¬¾åº”æ”¶", "é€€æ¬¾è¡Œæ•°"], ascending=False).head(30)
            st.markdown("**é€€æ¬¾Topèœå“ï¼ˆæŒ‰é€€æ¬¾åº”æ”¶ï¼‰**")
            st.dataframe(top_dish, use_container_width=True)
            st.bar_chart(top_dish.set_index("èœå“åç§°")[["é€€æ¬¾åº”æ”¶"]])

            top_slot = ref_rows.groupby("slot", as_index=False).agg(
                é€€æ¬¾åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"),
                é€€æ¬¾è®¢å•=("POSé”€å”®å•å·", "nunique"),
            ).sort_values("slot")
            st.markdown("**é€€æ¬¾æ—¶æ®µè¶‹åŠ¿ï¼ˆåŠå°æ—¶ï¼‰**")
            st.line_chart(top_slot.set_index("slot")[["é€€æ¬¾åº”æ”¶", "é€€æ¬¾è®¢å•"]])

            if "spec_norm" in ref_rows.columns:
                top_spec = ref_rows.groupby("spec_norm", as_index=False).agg(
                    é€€æ¬¾åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"),
                    é€€æ¬¾è¡Œæ•°=("èœå“åç§°", "count"),
                ).sort_values("é€€æ¬¾åº”æ”¶", ascending=False)
                st.markdown("**é€€æ¬¾æŒ‰è§„æ ¼**")
                st.dataframe(top_spec, use_container_width=True)

            if "categories" in ref_rows.columns:
                ex = ref_rows.copy().explode("categories")
                ex["categories"] = ex["categories"].fillna("æœªåˆ†ç±»")
                top_cat = ex.groupby("categories", as_index=False).agg(
                    é€€æ¬¾åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"),
                    é€€æ¬¾è¡Œæ•°=("èœå“åç§°", "count"),
                ).sort_values("é€€æ¬¾åº”æ”¶", ascending=False).head(30)
                st.markdown("**é€€æ¬¾æŒ‰å“ç±»**")
                st.dataframe(top_cat, use_container_width=True)

            if p is not None and not p.empty:
                pay_kind2 = p.groupby(["store_id", "POSé”€å”®å•å·"], as_index=False).agg(
                    k=("æ”¯ä»˜ç±»å‹", lambda s: "æ··åˆ" if s.nunique() > 1 else str(list(s)[0]))
                )
                ref_o = ref_rows[["store_id", "POSé”€å”®å•å·"]].drop_duplicates().merge(
                    pay_kind2, on=["store_id", "POSé”€å”®å•å·"], how="left"
                )
                ref_o["k"] = ref_o["k"].fillna("æœªçŸ¥")
                top_k = ref_o.groupby("k", as_index=False).agg(é€€æ¬¾è®¢å•=("POSé”€å”®å•å·", "nunique")).sort_values("é€€æ¬¾è®¢å•", ascending=False)
                st.markdown("**é€€æ¬¾è®¢å•æŒ‰æ”¯ä»˜æ¸ é“ï¼ˆè®¢å•å£å¾„ï¼‰**")
                st.dataframe(top_k, use_container_width=True)

            add_orders = set(filtered[sid]["items_add"]["order_id"].dropna().astype(str).tolist()) if (filtered[sid]["items_add"] is not None and not filtered[sid]["items_add"].empty) else set()
            refund_orders = set(ref_rows["POSé”€å”®å•å·"].dropna().astype(str).unique().tolist())
            has_add_rate = (len(refund_orders & add_orders) / len(refund_orders)) if refund_orders else 0.0
            st.metric("é€€æ¬¾è®¢å•å«å•åŠ å æ¯”", f"{has_add_rate*100:.1f}%")

            if add_orders and not filtered[sid]["items_add"].empty:
                a_ref = filtered[sid]["items_add"].copy()
                a_ref = a_ref[a_ref["order_id"].astype(str).isin(refund_orders)]
                if not a_ref.empty:
                    top_add_ref = a_ref.groupby("add_display", as_index=False).agg(
                        å•åŠ é‡‘é¢=("amount", "sum"),
                        å•åŠ è®¢å•=("order_id", "nunique"),
                    ).sort_values("å•åŠ é‡‘é¢", ascending=False).head(20)
                    st.markdown("**é€€æ¬¾è®¢å•ä¸­çš„å•åŠ Topï¼ˆæŒ‰å•åŠ é‡‘é¢ï¼‰**")
                    st.dataframe(top_add_ref, use_container_width=True)

            st.download_button(
                f"å¯¼å‡ºé€€æ¬¾æ˜ç»†ï¼ˆ{sid}ï¼‰CSV",
                key=_dl_key("ln696", sid) ,
                data=ref_rows.to_csv(index=False).encode("utf-8-sig"),
                file_name=f"é€€æ¬¾æ˜ç»†_{sid}.csv",
                mime="text/csv",
            )



    # â‘¦ é€€æ¬¾/å¼‚å¸¸ä¸å¯¹è´¦
    with tabs[6]:
        st.subheader("é€€æ¬¾/å¼‚å¸¸ä¸å¯¹è´¦ï¼šæŠ“é£é™©ã€æŠ“æ¼æŸã€æŠ“å£å¾„é—®é¢˜")
        st.caption("å¯¹è´¦å£å¾„ï¼šèœå“åº”æ”¶ï¼ˆä¼˜æƒ åå°è®¡æ±‚å’Œï¼‰ vs æ”¯ä»˜å®æ”¶ï¼ˆæ”¯ä»˜è¡¨é‡‘é¢æ±‚å’Œï¼‰ã€‚æ”¯æŒæŒ‰æ¸ é“/åŠå°æ—¶æ‹†åˆ†å·®å¼‚ï¼Œå¹¶åšé€€æ¬¾å½’å› ã€‚")

        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            o = filtered[sid]["orders"]
            p = filtered[sid]["pay"]
            if o.empty:
                st.info("æ— æ•°æ®")
                continue

            paid_by_order = (
                p.groupby(["store_id", "POSé”€å”®å•å·"], as_index=False).agg(paid=("æ€»é‡‘é¢", "sum"))
                if (p is not None and not p.empty)
                else pd.DataFrame(columns=["store_id", "POSé”€å”®å•å·", "paid"])
            )
            r = o.merge(paid_by_order, on=["store_id", "POSé”€å”®å•å·"], how="left")
            r["paid"] = r["paid"].fillna(0.0)
            r["diff"] = r["net_amount"] - r["paid"]

            c1, c2, c3 = st.columns(3)
            c1.metric("è®¢å•æ•°", int(r["POSé”€å”®å•å·"].nunique()))
            c2.metric("åº”æ”¶(ä¼˜æƒ å)", fmt_money(float(r["net_amount"].sum())))
            c3.metric("å®æ”¶", fmt_money(float(r["paid"].sum())))

            st.markdown("**å·®å¼‚Topè®¢å•ï¼ˆåº”æ”¶-å®æ”¶ï¼‰**")
            top_diff = r.sort_values("diff", ascending=False).head(200)
            st.dataframe(top_diff, use_container_width=True)
            st.download_button(
                f"å¯¼å‡ºå·®å¼‚Topè®¢å•ï¼ˆ{sid}ï¼‰CSV",
                key=_dl_key("ln736", sid) ,
                data=top_diff.to_csv(index=False).encode("utf-8-sig"),
                file_name=f"å¯¹è´¦å·®å¼‚Top_{sid}.csv",
                mime="text/csv",
            )

            if p is not None and not p.empty:
                pay_kind = p.groupby(["store_id", "POSé”€å”®å•å·"], as_index=False).agg(
                    paid=("æ€»é‡‘é¢", "sum"),
                    k=("æ”¯ä»˜ç±»å‹", lambda s: "æ··åˆ" if s.nunique() > 1 else str(list(s)[0])),
                )
                rr = o.merge(pay_kind, on=["store_id", "POSé”€å”®å•å·"], how="left")
                rr["paid"] = rr["paid"].fillna(0.0)
                rr["k"] = rr["k"].fillna("æœªçŸ¥")
                rr["diff"] = rr["net_amount"] - rr["paid"]

                byk = rr.groupby("k", as_index=False).agg(
                    è®¢å•æ•°=("POSé”€å”®å•å·", "nunique"),
                    åº”æ”¶=("net_amount", "sum"),
                    å®æ”¶=("paid", "sum"),
                    å·®å¼‚=("diff", "sum"),
                ).sort_values(["å·®å¼‚", "è®¢å•æ•°"], ascending=False)
                st.markdown("**æŒ‰æ”¯ä»˜æ¸ é“åˆ†è§£ï¼ˆå·®å¼‚=åº”æ”¶-å®æ”¶ï¼‰**")
                st.dataframe(byk, use_container_width=True)
                st.bar_chart(byk.set_index("k")[["å·®å¼‚"]])

                rr["slot"] = rr["order_time"].dt.floor("30min")
                bys = rr.groupby("slot", as_index=False).agg(
                    è®¢å•æ•°=("POSé”€å”®å•å·", "nunique"),
                    åº”æ”¶=("net_amount", "sum"),
                    å®æ”¶=("paid", "sum"),
                    å·®å¼‚=("diff", "sum"),
                ).sort_values("slot")
                st.markdown("**æŒ‰åŠå°æ—¶åˆ†è§£ï¼ˆå·®å¼‚è¶‹åŠ¿ï¼‰**")
                st.line_chart(bys.set_index("slot")[["å·®å¼‚", "åº”æ”¶", "å®æ”¶"]])

            refund_rate = float(o["has_refund"].mean()) if ("has_refund" in o.columns and not o.empty) else 0.0
            st.metric("é€€æ¬¾å•å æ¯”ï¼ˆèœå“è¡¨å­˜åœ¨POSé€€æ¬¾å•å·ï¼‰", f"{refund_rate * 100:.1f}%")

            st.markdown("### é€€æ¬¾å½’å› ï¼šèœå“/æ—¶æ®µ/æ¸ é“/è§„æ ¼/å“ç±»ï¼ˆå¯å¯¼å‡ºï¼‰")
            items = filtered[sid]["items_main"]
            ref_rows = (
                items[items["POSé€€æ¬¾å•å·"].notna()].copy()
                if (items is not None and not items.empty and "POSé€€æ¬¾å•å·" in items.columns)
                else pd.DataFrame()
            )
            if ref_rows.empty:
                st.info("è¯¥æ—¶é—´èŒƒå›´å†…æœªè¯†åˆ«åˆ°é€€æ¬¾è¡Œï¼ˆPOSé€€æ¬¾å•å· ä¸ºç©ºï¼‰ã€‚")
                continue

            ref_rows["slot"] = ref_rows["åˆ›å»ºæ—¶é—´"].dt.floor("30min")

            top_dish = ref_rows.groupby("èœå“åç§°", as_index=False).agg(
                é€€æ¬¾è¡Œæ•°=("èœå“åç§°", "count"),
                é€€æ¬¾åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"),
                é€€æ¬¾è®¢å•=("POSé”€å”®å•å·", "nunique"),
            ).sort_values(["é€€æ¬¾åº”æ”¶", "é€€æ¬¾è¡Œæ•°"], ascending=False).head(30)
            st.markdown("**é€€æ¬¾Topèœå“ï¼ˆæŒ‰é€€æ¬¾åº”æ”¶ï¼‰**")
            st.dataframe(top_dish, use_container_width=True)
            st.bar_chart(top_dish.set_index("èœå“åç§°")[["é€€æ¬¾åº”æ”¶"]])

            top_slot = ref_rows.groupby("slot", as_index=False).agg(
                é€€æ¬¾åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"),
                é€€æ¬¾è®¢å•=("POSé”€å”®å•å·", "nunique"),
            ).sort_values("slot")
            st.markdown("**é€€æ¬¾æ—¶æ®µè¶‹åŠ¿ï¼ˆåŠå°æ—¶ï¼‰**")
            st.line_chart(top_slot.set_index("slot")[["é€€æ¬¾åº”æ”¶", "é€€æ¬¾è®¢å•"]])

            if "spec_norm" in ref_rows.columns:
                top_spec = ref_rows.groupby("spec_norm", as_index=False).agg(
                    é€€æ¬¾åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"),
                    é€€æ¬¾è¡Œæ•°=("èœå“åç§°", "count"),
                ).sort_values("é€€æ¬¾åº”æ”¶", ascending=False)
                st.markdown("**é€€æ¬¾æŒ‰è§„æ ¼**")
                st.dataframe(top_spec, use_container_width=True)

            if "categories" in ref_rows.columns:
                ex = ref_rows.copy().explode("categories")
                ex["categories"] = ex["categories"].fillna("æœªåˆ†ç±»")
                top_cat = ex.groupby("categories", as_index=False).agg(
                    é€€æ¬¾åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"),
                    é€€æ¬¾è¡Œæ•°=("èœå“åç§°", "count"),
                ).sort_values("é€€æ¬¾åº”æ”¶", ascending=False).head(30)
                st.markdown("**é€€æ¬¾æŒ‰å“ç±»**")
                st.dataframe(top_cat, use_container_width=True)

            if p is not None and not p.empty:
                pay_kind2 = p.groupby(["store_id", "POSé”€å”®å•å·"], as_index=False).agg(
                    k=("æ”¯ä»˜ç±»å‹", lambda s: "æ··åˆ" if s.nunique() > 1 else str(list(s)[0]))
                )
                ref_o = ref_rows[["store_id", "POSé”€å”®å•å·"]].drop_duplicates().merge(
                    pay_kind2, on=["store_id", "POSé”€å”®å•å·"], how="left"
                )
                ref_o["k"] = ref_o["k"].fillna("æœªçŸ¥")
                top_k = ref_o.groupby("k", as_index=False).agg(é€€æ¬¾è®¢å•=("POSé”€å”®å•å·", "nunique")).sort_values("é€€æ¬¾è®¢å•", ascending=False)
                st.markdown("**é€€æ¬¾è®¢å•æŒ‰æ”¯ä»˜æ¸ é“ï¼ˆè®¢å•å£å¾„ï¼‰**")
                st.dataframe(top_k, use_container_width=True)

            add_orders = set(filtered[sid]["items_add"]["order_id"].dropna().astype(str).tolist()) if (filtered[sid]["items_add"] is not None and not filtered[sid]["items_add"].empty) else set()
            refund_orders = set(ref_rows["POSé”€å”®å•å·"].dropna().astype(str).unique().tolist())
            has_add_rate = (len(refund_orders & add_orders) / len(refund_orders)) if refund_orders else 0.0
            st.metric("é€€æ¬¾è®¢å•å«å•åŠ å æ¯”", f"{has_add_rate*100:.1f}%")

            if add_orders and not filtered[sid]["items_add"].empty:
                a_ref = filtered[sid]["items_add"].copy()
                a_ref = a_ref[a_ref["order_id"].astype(str).isin(refund_orders)]
                if not a_ref.empty:
                    top_add_ref = a_ref.groupby("add_display", as_index=False).agg(
                        å•åŠ é‡‘é¢=("amount", "sum"),
                        å•åŠ è®¢å•=("order_id", "nunique"),
                    ).sort_values("å•åŠ é‡‘é¢", ascending=False).head(20)
                    st.markdown("**é€€æ¬¾è®¢å•ä¸­çš„å•åŠ Topï¼ˆæŒ‰å•åŠ é‡‘é¢ï¼‰**")
                    st.dataframe(top_add_ref, use_container_width=True)

            st.download_button(
                f"å¯¼å‡ºé€€æ¬¾æ˜ç»†ï¼ˆ{sid}ï¼‰CSV",
                key=_dl_key("ln852", sid) ,
                data=ref_rows.to_csv(index=False).encode("utf-8-sig"),
                file_name=f"é€€æ¬¾æ˜ç»†_{sid}.csv",
                mime="text/csv",
            )

    # â‘§ æœªåˆ†ç±»æ± 
    with tabs[7]:
        st.subheader("æœªåˆ†ç±»æ± ï¼šå¯æŸ¥çœ‹ã€å¯å¯¼å‡ºï¼ˆè§„åˆ™è¿­ä»£å…¥å£ï¼‰")
        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            m = filtered[sid]["items_main"]
            if m.empty:
                st.info("æ— æ•°æ®")
                continue
            un = m[m["categories"].apply(lambda x: len(x) == 0)].copy()
            st.write(f"æœªåˆ†ç±»ä¸»èœè¡Œæ•°ï¼š{len(un):,}")
            st.dataframe(un.head(200), use_container_width=True)
            key=_dl_key("ln872", sid) ,
            st.download_button(f"å¯¼å‡ºæœªåˆ†ç±»ä¸»èœï¼ˆ{sid}ï¼‰CSV", data=un.to_csv(index=False).encode("utf-8-sig"), file_name=f"æœªåˆ†ç±»ä¸»èœ_{sid}.csv", mime="text/csv")

    # â‘¨ æ˜ç»†å¯¼å‡º
    with tabs[8]:
        st.subheader("æ˜ç»†å¯¼å‡ºï¼šæ€»éƒ¨/è´¢åŠ¡/åº—é•¿äºŒæ¬¡åˆ†æ")
        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            m = filtered[sid]["items_main"]
            key=_dl_key("ln880", sid) ,
            st.download_button(f"å¯¼å‡ºèœå“æ˜ç»†-è¿‡æ»¤åï¼ˆ{sid}ï¼‰CSV", data=m.to_csv(index=False).encode("utf-8-sig"), file_name=f"èœå“æ˜ç»†_è¿‡æ»¤å_{sid}.csv", mime="text/csv")

    # â‘© æ—¶æ®µçƒ­åŠ›å›¾ï¼ˆåŠ¨ä½œå»ºè®®+å¯¼å‡ºï¼‰
    with tabs[9]:
        st.subheader("æ—¶æ®µçƒ­åŠ›å›¾ï¼ˆ30åˆ†é’Ÿç²’åº¦ï¼‰ï¼šå³°è°·ã€æ’ç­ã€å¤‡è´§ã€æ¸ é“åŠ¨ä½œ")
        st.caption("è¡Œ=æ—¥æœŸï¼Œåˆ—=åŠå°æ—¶ï¼›å¯é€‰æ‹©æŒ‡æ ‡ï¼›æ”¯æŒé€‰ä¸­é—¨åº—æ±‡æ€»æˆ–æŒ‰é—¨åº—åˆ†åˆ«æŸ¥çœ‹ã€‚")

        metric = st.selectbox("é€‰æ‹©æŒ‡æ ‡", options=["è®¢å•æ•°", "åº”æ”¶(ä¼˜æƒ å)", "å®æ”¶", "å®¢å•(åº”æ”¶/è®¢å•)", "å•åŠ æ¸—é€ç‡(å«å•åŠ è®¢å•/æ€»è®¢å•)"], index=0)
        scope = st.radio("èŒƒå›´", options=["é€‰ä¸­é—¨åº—æ±‡æ€»", "æŒ‰é—¨åº—åˆ†åˆ«çœ‹"], horizontal=True)

        def _build_heat(o_df: pd.DataFrame, p_df: pd.DataFrame, a_df: pd.DataFrame) -> Optional[pd.DataFrame]:
            if o_df is None or o_df.empty:
                return None
            o = o_df.copy()
            o["date"] = o["order_time"].dt.date
            o["slot"] = o["order_time"].dt.floor("30min").dt.strftime("%H:%M")
            grp = o.groupby(["date", "slot"], as_index=False).agg(orders=("POSé”€å”®å•å·", "nunique"), net=("net_amount", "sum"))
            if p_df is not None and not p_df.empty:
                p = p_df.copy()
                p["date"] = p["order_time"].dt.date
                p["slot"] = p["order_time"].dt.floor("30min").dt.strftime("%H:%M")
                paid = p.groupby(["date", "slot"], as_index=False).agg(paid=("æ€»é‡‘é¢", "sum"))
                grp = grp.merge(paid, on=["date", "slot"], how="left")
            grp["paid"] = grp.get("paid", 0).fillna(0.0)
            if a_df is not None and not a_df.empty:
                a = a_df.copy()
                a["date"] = a["created_at"].dt.date
                a["slot"] = a["created_at"].dt.floor("30min").dt.strftime("%H:%M")
                add_o = a.groupby(["date", "slot"], as_index=False).agg(add_orders=("order_id", "nunique"))
                grp = grp.merge(add_o, on=["date", "slot"], how="left")
            grp["add_orders"] = grp.get("add_orders", 0).fillna(0)
            grp["aov"] = grp["net"] / grp["orders"].replace(0, np.nan)
            grp["add_rate"] = grp["add_orders"] / grp["orders"].replace(0, np.nan)
            return grp

        def _render_heat(df: Optional[pd.DataFrame], key_prefix: str) -> None:
            if df is None or df.empty:
                st.info("æ— å¯ç”¨æ•°æ®")
                return
            if metric == "è®¢å•æ•°":
                mat = df.pivot(index="date", columns="slot", values="orders").fillna(0).astype(int)
            elif metric == "åº”æ”¶(ä¼˜æƒ å)":
                mat = df.pivot(index="date", columns="slot", values="net").fillna(0.0)
            elif metric == "å®æ”¶":
                mat = df.pivot(index="date", columns="slot", values="paid").fillna(0.0)
            elif metric == "å®¢å•(åº”æ”¶/è®¢å•)":
                mat = df.pivot(index="date", columns="slot", values="aov")
            else:
                mat = df.pivot(index="date", columns="slot", values="add_rate")
            cols = sorted(mat.columns, key=lambda x: (int(x.split(":")[0]), int(x.split(":")[1])))
            mat = mat[cols]

            view = st.radio("å±•ç¤ºæ–¹å¼", options=["çƒ­åŠ›å›¾", "æ¸å˜è¡¨æ ¼"], horizontal=True, key=f"heat_view_{key_prefix}_{metric}")
            if view == "æ¸å˜è¡¨æ ¼":
                st.dataframe(_simple_gradient_styler(mat), use_container_width=True)
            else:
                mdf = mat.reset_index().melt(id_vars="date", var_name="slot", value_name="value")
                chart = alt.Chart(mdf).mark_rect().encode(
                    x=alt.X("slot:N", title="åŠå°æ—¶"),
                    y=alt.Y("date:N", title="æ—¥æœŸ"),
                    color=alt.Color("value:Q", title=metric),
                    tooltip=["date", "slot", "value"],
                ).properties(height=320)
                st.altair_chart(chart, use_container_width=True)

        if scope == "é€‰ä¸­é—¨åº—æ±‡æ€»":
            oall2 = pd.concat([filtered[s]["orders"] for s in sel_stores], ignore_index=True)
            pall2 = pd.concat([filtered[s]["pay"] for s in sel_stores], ignore_index=True)
            aall2 = pd.concat([filtered[s]["items_add"] for s in sel_stores], ignore_index=True)
            baseh = _build_heat(oall2, pall2, aall2)
            _render_heat(baseh, "all")

            st.markdown("### åŠ¨ä½œå»ºè®®ï¼ˆA2ï¼‰ï¼šå³°è°·æ’ç­ã€ä¿ƒé”€æ—¶æ®µã€å•åŠ å¼•å¯¼")
            if baseh is not None and not baseh.empty:
                agg = baseh.groupby("slot", as_index=False).agg(è®¢å•æ•°=("orders", "sum"), åº”æ”¶=("net", "sum"), å®æ”¶=("paid", "sum"), å•åŠ è®¢å•=("add_orders", "sum"))
                agg["å®¢å•"] = agg["åº”æ”¶"] / agg["è®¢å•æ•°"].replace(0, np.nan)
                agg["å•åŠ æ¸—é€ç‡"] = agg["å•åŠ è®¢å•"] / agg["è®¢å•æ•°"].replace(0, np.nan)
                peak = agg.sort_values("è®¢å•æ•°", ascending=False).head(5)
                low = agg[agg["è®¢å•æ•°"] > 0].sort_values("è®¢å•æ•°", ascending=True).head(5)
                opp = agg[agg["è®¢å•æ•°"] >= agg["è®¢å•æ•°"].median()].sort_values("å•åŠ æ¸—é€ç‡", ascending=True).head(5)

                st.dataframe(pd.concat([peak.assign(ç±»å‹="å³°å€¼"), low.assign(ç±»å‹="ä½è°·"), opp.assign(ç±»å‹="å•åŠ æœºä¼š")], ignore_index=True), use_container_width=True)
                action = pd.concat([peak.assign(å»ºè®®="å³°å€¼ï¼šåŠ äºº/å¤‡è´§/ä¿å‡ºé¤"), low.assign(å»ºè®®="ä½è°·ï¼šä¿ƒé”€/å›¢è´­/å¼•å¯¼å•åŠ "), opp.assign(å»ºè®®="æœºä¼šï¼šå¼ºåŒ–å•åŠ è¯æœ¯")], ignore_index=True)
                key=_dl_key("ln963", sid) ,
                st.download_button("å¯¼å‡ºåº—é•¿è¡ŒåŠ¨æ¸…å• CSV", data=action.to_csv(index=False).encode("utf-8-sig"), file_name="åº—é•¿è¡ŒåŠ¨æ¸…å•.csv", mime="text/csv")
            else:
                st.info("åŠ¨ä½œå»ºè®®ï¼šå½“å‰èŒƒå›´æ— è¶³å¤Ÿæ•°æ®ã€‚")
        else:
            for sid in sel_stores:
                st.markdown(f"#### é—¨åº— {sid}")
                dfh = _build_heat(filtered[sid]["orders"], filtered[sid]["pay"], filtered[sid]["items_add"])
                _render_heat(dfh, sid)

    # â‘ª é—¨åº—ç”»åƒå¡ï¼ˆä¸¤åº—å¯¹æ¯”ï¼‰
    with tabs[10]:
        st.subheader("é—¨åº—ç”»åƒå¡ï¼ˆä¸¤åº—å¯¹æ¯”ï¼‰ï¼šä¸€é¡µåˆ¤æ–­â€œåƒè° / ååœ¨å“ª / æ€ä¹ˆæ”¹â€")
        st.caption("é€‰æ‹©ä¸¤å®¶é—¨åº—è¿›è¡Œå¯¹æ¯”ï¼šKPIå¯¹ç…§ + é›·è¾¾å›¾ï¼ˆèƒ½åŠ›ç»“æ„ï¼‰+ åç¦»åº¦æ‹†è§£ï¼ˆè§„æ ¼/å•åŠ /æ”¯ä»˜/å“ç±»/æ—¶æ®µï¼‰+ è¡ŒåŠ¨å»ºè®®ã€‚")

        if len(sel_stores) < 2:
            st.info("è¯·åœ¨é¡¶éƒ¨â€œé€‰æ‹©é—¨åº—â€ä¸­è‡³å°‘é€‰æ‹© 2 å®¶é—¨åº—ï¼Œæ‰èƒ½è¿›è¡Œä¸¤åº—å¯¹æ¯”ã€‚")
        else:
            c1, c2, c3 = st.columns([2, 2, 2])
            with c1:
                store_a = st.selectbox("é—¨åº—A", options=sel_stores, index=0, key="portrait_a")
            with c2:
                opts_b = [s for s in sel_stores if s != store_a] or sel_stores
                store_b = st.selectbox("é—¨åº—B", options=opts_b, index=0, key="portrait_b")
            with c3:
                bench_mode = st.selectbox("åŸºå‡†", options=["ä¸¤åº—å‡å€¼", "ä»¥é—¨åº—Aä¸ºæ ‡æ†", "ä»¥é—¨åº—Bä¸ºæ ‡æ†"], index=0, key="portrait_bench")

            fa = filtered[store_a]
            fb = filtered[store_b]

            sa = _compute_profile_scores(fa)
            sb = _compute_profile_scores(fb)

            kpi_cols = ["è®¢å•æ•°", "åº”æ”¶", "å®æ”¶", "å®¢å•", "é€€æ¬¾ç‡", "å¯¹è´¦å·®å¼‚ç‡", "å•åŠ æ¸—é€ç‡", "å•åŠ é‡‘é¢å æ¯”", "è§„æ ¼å¤šæ ·æ€§", "æ¸ é“å¤šæ ·æ€§", "æ¸ é“æœ€å¤§å æ¯”", "å“ç±»Top5å æ¯”", "å³°å€¼Top3å æ¯”"]
            kpidf = pd.DataFrame({"æŒ‡æ ‡": kpi_cols, store_a: [sa.get(k) for k in kpi_cols], store_b: [sb.get(k) for k in kpi_cols]})
            st.dataframe(kpidf, use_container_width=True)

            radar_dims = ["å®¢å•", "å•åŠ æ¸—é€ç‡", "è§„æ ¼å¤šæ ·æ€§", "æ¸ é“å¤šæ ·æ€§", "å“ç±»Top5å æ¯”", "å³°å€¼Top3å æ¯”", "é€€æ¬¾ç‡", "å¯¹è´¦å·®å¼‚ç‡"]
            invert = {"å“ç±»Top5å æ¯”", "å³°å€¼Top3å æ¯”", "é€€æ¬¾ç‡", "å¯¹è´¦å·®å¼‚ç‡"}
            # ç”¨â€œæœ¬æ¬¡ç­›é€‰é—¨åº—çš„åˆ†å¸ƒâ€åšå½’ä¸€åŒ–ï¼Œé¿å…ä¸¤åº—äº’ç›¸å½“æ ‡å°ºå¯¼è‡´é›·è¾¾å›¾å¤±çœŸ
            profiles = {}
            for _sid in sel_stores:
                try:
                    profiles[_sid] = _compute_profile_scores(filtered[_sid])
                except Exception:
                    profiles[_sid] = {}

            normalize_max = {}
            for d in radar_dims:
                vals = [float(profiles.get(_sid, {}).get(d, 0.0) or 0.0) for _sid in sel_stores]
                mx = float(np.nanpercentile(vals, 95)) if len(vals) else 1.0
                normalize_max[d] = max(mx, 1e-9)


            dfa = _radar_df(sa, radar_dims, normalize_max, invert); dfa["é—¨åº—"] = store_a
            dfb = _radar_df(sb, radar_dims, normalize_max, invert); dfb["é—¨åº—"] = store_b
            rdf = pd.concat([dfa, dfb], ignore_index=True)

            st.markdown("### èƒ½åŠ›ç»“æ„é›·è¾¾å›¾ï¼ˆ0-1ï¼‰ï¼šè¶Šå¤–åœˆè¶Šå¼º")
            st.altair_chart(_radar_chart(rdf, store_col="é—¨åº—"), use_container_width=True)

            st.markdown("### åç¦»åº¦æ‹†è§£ï¼ˆJS divergenceï¼‰ï¼šå‘Šè¯‰ä½ â€œååœ¨å“ªâ€")

            def bench_dist(keys_a, vals_a, keys_b, vals_b):
                _, a, b = _align_two(keys_a, vals_a, keys_b, vals_b)
                if bench_mode == "ä»¥é—¨åº—Aä¸ºæ ‡æ†":
                    q = a
                elif bench_mode == "ä»¥é—¨åº—Bä¸ºæ ‡æ†":
                    q = b
                else:
                    q = 0.5 * (a + b)
                return a, b, q

            ma = _base_items(fa["items_main"]); mb = _base_items(fb["items_main"])
            spec_a = ma[ma["spec_norm"].notna()] if (ma is not None and not ma.empty and "spec_norm" in ma.columns) else pd.DataFrame()
            spec_b = mb[mb["spec_norm"].notna()] if (mb is not None and not mb.empty and "spec_norm" in mb.columns) else pd.DataFrame()
            k_sa, v_sa = _dist_from_group(spec_a, "spec_norm", "èœå“æ•°é‡", topn=10)
            k_sb, v_sb = _dist_from_group(spec_b, "spec_norm", "èœå“æ•°é‡", topn=10)
            va, vb, q = bench_dist(k_sa, v_sa, k_sb, v_sb)
            js_spec_a, js_spec_b = _js_divergence(va, q), _js_divergence(vb, q)

            k_aa, v_aa = _dist_from_group(fa["items_add"], "add_display", "amount", topn=15)
            k_ab, v_ab = _dist_from_group(fb["items_add"], "add_display", "amount", topn=15)
            va2, vb2, q2 = bench_dist(k_aa, v_aa, k_ab, v_ab)
            js_add_a, js_add_b = _js_divergence(va2, q2), _js_divergence(vb2, q2)

            k_pa, v_pa = _dist_from_group(fa["pay"], "æ”¯ä»˜ç±»å‹", "æ€»é‡‘é¢", topn=15)
            k_pb, v_pb = _dist_from_group(fb["pay"], "æ”¯ä»˜ç±»å‹", "æ€»é‡‘é¢", topn=15)
            va3, vb3, q3 = bench_dist(k_pa, v_pa, k_pb, v_pb)
            js_pay_a, js_pay_b = _js_divergence(va3, q3), _js_divergence(vb3, q3)

            def cat_dist(x):
                if x is None or x.empty or "categories" not in x.columns:
                    return [], np.array([], dtype=float)
                ex = x.copy().explode("categories"); ex["categories"] = ex["categories"].fillna("æœªåˆ†ç±»")
                return _dist_from_group(ex, "categories", "ä¼˜æƒ åå°è®¡ä»·æ ¼", topn=25)

            k_ca, v_ca = cat_dist(fa["items_main"]); k_cb, v_cb = cat_dist(fb["items_main"])
            va4, vb4, q4 = bench_dist(k_ca, v_ca, k_cb, v_cb)
            js_cat_a, js_cat_b = _js_divergence(va4, q4), _js_divergence(vb4, q4)

            def slot_dist(o):
                if o is None or o.empty:
                    return [], np.array([], dtype=float)
                t = o.copy()
                t["slot"] = t["order_time"].dt.floor("30min").dt.strftime("%H:%M")
                return _dist_from_group(t, "slot", "dish_qty", topn=48)

            k_ta, v_ta = slot_dist(fa["orders"]); k_tb, v_tb = slot_dist(fb["orders"])
            va5, vb5, q5 = bench_dist(k_ta, v_ta, k_tb, v_tb)
            js_time_a, js_time_b = _js_divergence(va5, q5), _js_divergence(vb5, q5)

            dims = ["è§„æ ¼", "å•åŠ ", "æ”¯ä»˜", "å“ç±»", "æ—¶æ®µ"]
            js_a = np.array([js_spec_a, js_add_a, js_pay_a, js_cat_a, js_time_a], dtype=float)
            js_b = np.array([js_spec_b, js_add_b, js_pay_b, js_cat_b, js_time_b], dtype=float)

            suma = float(js_a.sum()) if float(js_a.sum()) > 0 else 1.0
            sumb = float(js_b.sum()) if float(js_b.sum()) > 0 else 1.0

            div_df = pd.DataFrame({
                "ç»´åº¦": dims,
                f"{store_a} åç¦»åº¦": js_a,
                f"{store_a} è´¡çŒ®": js_a / suma,
                f"{store_b} åç¦»åº¦": js_b,
                f"{store_b} è´¡çŒ®": js_b / sumb,
            })
            st.dataframe(div_df.style.format({f"{store_a} åç¦»åº¦": "{:.4f}", f"{store_b} åç¦»åº¦": "{:.4f}", f"{store_a} è´¡çŒ®": "{:.0%}", f"{store_b} è´¡çŒ®": "{:.0%}"}), use_container_width=True)

            def top_reason(js_vec: np.ndarray) -> str:
                if float(js_vec.sum()) <= 0:
                    return "ç»“æ„ä¸åŸºå‡†å‡ ä¹ä¸€è‡´"
                k = int(np.argmax(js_vec))
                return f"ä¸»è¦åç¦»æ¥è‡ªã€{dims[k]}ã€‘ï¼ˆè´¡çŒ® {js_vec[k]/js_vec.sum():.0%}ï¼‰"

            colx, coly = st.columns(2)
            with colx:
                st.markdown(f"**{store_a} ç»“è®ºï¼š** {top_reason(js_a)}")
            with coly:
                st.markdown(f"**{store_b} ç»“è®ºï¼š** {top_reason(js_b)}")

            st.markdown("### è¡ŒåŠ¨å»ºè®®")
            def advice(scores: Dict[str, float], radar_scores: pd.DataFrame) -> List[str]:
                # å…ˆè·‘â€œç¡¬è§„åˆ™â€å‘Šè­¦ï¼Œå†è¡¥â€œçŸ­æ¿Top2â€å»ºè®®ï¼Œä¿è¯æ¯å®¶åº—éƒ½æœ‰å¯æ‰§è¡ŒåŠ¨ä½œ
                out: List[str] = []

                if scores.get("é€€æ¬¾ç‡", 0) > 0.03:
                    out.append("é€€æ¬¾åé«˜ï¼šä¼˜å…ˆæ’æŸ¥ Topé€€æ¬¾èœå“ä¸å¯¹åº”æ—¶æ®µï¼Œæ ¸æŸ¥å‡ºå“ç¨³å®šæ€§/é…é€é—®é¢˜/æ”¯ä»˜å¯¹è´¦å£å¾„ã€‚")
                if scores.get("å¯¹è´¦å·®å¼‚ç‡", 0) > 0.02:
                    out.append("å¯¹è´¦å·®å¼‚åå¤§ï¼šé‡ç‚¹çœ‹æ··åˆæ”¯ä»˜ä¸å›¢è´­æ¸ é“ï¼Œæ’æŸ¥æ¼è®°/é€€æ¬¾å£å¾„å·®/è·¨æ—¥æ”¯ä»˜ã€‚")
                if scores.get("æ¸ é“æœ€å¤§å æ¯”", 0) > 0.75:
                    out.append("æ¸ é“è¿‡åº¦ä¾èµ–ï¼šæ³¨æ„å•ä¸€æ¸ é“æ³¢åŠ¨é£é™©ï¼Œä¼˜åŒ–å¤šæ¸ é“æ¸—é€ï¼ˆå›¢è´­/å°ç¨‹åº/å¤–å–ï¼‰ã€‚")

                weakest = radar_scores.sort_values("å¾—åˆ†", ascending=True).head(2)["ç»´åº¦"].tolist()
                for w in weakest:
                    if w == "å•åŠ æ¸—é€ç‡":
                        out.append("å•åŠ åå¼±ï¼šé«˜å³°æ—¶æ®µå¼ºåŒ–â€œåŠ æ–™è¯æœ¯/æç¤ºå¡â€ï¼Œé‡ç‚¹æå‡å•åŠ -é¸¡ä¸/å•åŠ -å¤è›‹ç­‰æ¸—é€ã€‚")
                    elif w == "è§„æ ¼å¤šæ ·æ€§":
                        out.append("è§„æ ¼ç»“æ„åå•ä¸€ï¼šæ£€æŸ¥ä¸»é£Ÿç»“æ„ï¼ˆç»†é¢/å®½é¢/ç±³é¥­/å®½ç²‰/æ— éœ€ä¸»é£Ÿ/å¥—é¤æ ‡å‡†ï¼‰æ˜¯å¦å¤±è¡¡ï¼Œå°è¯•ç”¨å¥—é¤ä¸é™ˆåˆ—å¼•å¯¼åˆ†æµã€‚")
                    elif w == "æ¸ é“å¤šæ ·æ€§":
                        out.append("æ”¯ä»˜æ¸ é“è¿‡äºå•ä¸€ï¼šæ£€æŸ¥çº¿ä¸Š/çº¿ä¸‹ã€å›¢è´­æ¸—é€ä¸æ”¯ä»˜æ–¹å¼è¦†ç›–ï¼Œå‡å°‘å•ç‚¹æ³¢åŠ¨ã€‚")
                    elif w == "å“ç±»Top5å æ¯”":
                        out.append("å“ç±»è¿‡åº¦é›†ä¸­ï¼šæ£€æŸ¥çˆ†å“ä¾èµ–ä¸ç¼ºè´§é£é™©ï¼Œç”¨å¥—é¤/ç¬¬äºŒçˆ†å“/æ­é…å•åŠ åˆ†æµã€‚")
                    elif w == "å³°å€¼Top3å æ¯”":
                        out.append("å³°å€¼è¿‡åº¦é›†ä¸­ï¼šåœ¨Top3åŠå°æ—¶åŠ äºº/å¤‡è´§ï¼›ä½è°·ç”¨å›¢è´­ã€å°å¥—é¤ã€ä¸»é£Ÿç»„åˆæ‹‰å¹³æ³¢åŠ¨ã€‚")
                    elif w == "å®¢å•":
                        out.append("å®¢å•åä½ï¼šç”¨å¥—é¤æ ‡å‡†ã€å•åŠ æ¨èä¸é«˜æ¯›åˆ©é¥®å“/å°é£Ÿæå‡å®¢å•ã€‚")
                    elif w == "é€€æ¬¾ç‡":
                        out.append("é€€æ¬¾åé«˜ï¼šä¼˜å…ˆçœ‹é€€æ¬¾çƒ­åŠ›æ—¶æ®µä¸Topé€€æ¬¾èœå“ï¼Œå®šä½æµç¨‹æˆ–æ¸ é“é—®é¢˜ã€‚")
                    elif w == "å¯¹è´¦å·®å¼‚ç‡":
                        out.append("å¯¹è´¦å·®å¼‚åå¤§ï¼šé‡ç‚¹æ ¸å¯¹æ”¯ä»˜è¡¨ä¸è®¢å•è¡¨æ—¶é—´å£å¾„ã€è·¨æ—¥æ”¯ä»˜ä¸é€€æ¬¾å½’å±ã€‚")

                seen = set()
                uniq = []
                for s in out:
                    if s not in seen:
                        uniq.append(s); seen.add(s)
                return uniq[:6] if uniq else ["ç»“æ„å¥åº·ï¼šå»ºè®®ä½œä¸ºæ ‡æ†é—¨åº—ï¼Œæ²‰æ·€å¯å¤åˆ¶æ‰“æ³•ï¼ˆèœå•ç»“æ„/å•åŠ /æ¸ é“/æ’ç­ï¼‰ã€‚"]

            a1, a2 = st.columns(2)
            with a1:
                st.markdown(f"**{store_a} å»ºè®®**")
                for s in advice(sa, dfa):
                    st.write("â€¢ " + s)
            with a2:
                st.markdown(f"**{store_b} å»ºè®®**")
                for s in advice(sb, dfb):
                    st.write("â€¢ " + s)

            st.download_button(
                "å¯¼å‡ºç”»åƒå¡æŒ‡æ ‡å¯¹æ¯” CSV",
                data=kpidf.to_csv(index=False).encode("utf-8-sig"),
                file_name=f"é—¨åº—ç”»åƒå¡å¯¹æ¯”_{store_a}_vs_{store_b}.csv",
                mime="text/csv",
                key=_dl_key("portrait"),
            )




if __name__ == "__main__":
    main()