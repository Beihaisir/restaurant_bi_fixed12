from __future__ import annotations

import io
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import streamlit as st
import altair as alt

from utils.io import read_any_table, read_raw_noheader
from utils.rules import load_rules_xlsx, match_categories, Rule
from utils.transform import build_fact_tables

st.set_page_config(page_title="é¤é¥®ç»è¥åˆ†æç³»ç»Ÿ", layout="wide")


@dataclass
class StoreBundle:
    store_id: str
    daily: Optional[pd.DataFrame]
    dish: Optional[pd.DataFrame]
    pay: Optional[pd.DataFrame]


DAILY_MUST = ["é—¨åº—ä»£ç ", "é—¨åº—åç§°", "æ—¥æœŸ"]
DISH_MUST = ["åˆ›å»ºæ—¶é—´", "èœå“åç§°", "POSé”€å”®å•å·"]
PAY_MUST = ["POSé”€å”®å•å·", "æ”¯ä»˜ç±»å‹", "æ€»é‡‘é¢"]


def _as_bytes(uploaded) -> bytes:
    return uploaded.getvalue()


def _norm_colset(cols) -> set:
    out = set()
    for c in cols:
        s = str(c).strip().replace(" ", "").replace("\u3000", "")
        out.add(s)
    return out


def detect_table_kind(file_bytes: bytes, filename: str) -> Tuple[str, Optional[str]]:
    raw = read_raw_noheader(file_bytes, filename)

    store_id = None
    for r in range(min(20, len(raw))):
        for cell in raw.iloc[r].astype(str).tolist():
            if "å¯¼å‡ºäºº" in str(cell):
                import re

                m = re.search(r"å¯¼å‡ºäºº[:ï¼š]\s*(\d+)", str(cell))
                if m:
                    store_id = m.group(1)
                    break
        if store_id:
            break

    top_text = " ".join(raw.head(5).astype(str).fillna("").values.flatten().tolist())
    if "æ—¥é”€å”®æŠ¥è¡¨" in top_text:
        return "daily", store_id

    df_dish, _ = read_any_table(file_bytes, filename, DISH_MUST)
    if {"POSé”€å”®å•å·", "èœå“åç§°", "åˆ›å»ºæ—¶é—´"}.issubset(_norm_colset(df_dish.columns)):
        return "dish", store_id

    df_pay, _ = read_any_table(file_bytes, filename, PAY_MUST)
    if {"POSé”€å”®å•å·", "æ”¯ä»˜ç±»å‹"}.issubset(_norm_colset(df_pay.columns)):
        return "pay", store_id

    df_daily, _ = read_any_table(file_bytes, filename, DAILY_MUST)
    cols_daily = _norm_colset(df_daily.columns)
    if ("å«ç¨é”€å”®é¢" in cols_daily) or ("å®¢æµé‡" in cols_daily) or ({"é—¨åº—ä»£ç ", "é—¨åº—åç§°", "æ—¥æœŸ"}.issubset(cols_daily)):
        return "daily", store_id

    return "unknown", store_id


@st.cache_data(show_spinner=False)
def parse_uploaded(files: List, rule_file) -> Tuple[List[StoreBundle], List[str], List[Rule]]:
    rules: List[Rule] = []
    if rule_file is not None:
        rules = load_rules_xlsx(io.BytesIO(_as_bytes(rule_file)))

    bundles: Dict[str, StoreBundle] = {}
    warnings: List[str] = []

    def upsert(store_id: str) -> StoreBundle:
        if store_id not in bundles:
            bundles[store_id] = StoreBundle(store_id=store_id, daily=None, dish=None, pay=None)
        return bundles[store_id]

    for f in files:
        b = _as_bytes(f)
        name = f.name
        kind, store_id = detect_table_kind(b, name)
        if store_id is None:
            store_id = "UNKNOWN"

        if kind == "daily":
            df, _ = read_any_table(b, name, DAILY_MUST)
            upsert(store_id).daily = df
        elif kind == "dish":
            df, _ = read_any_table(b, name, DISH_MUST)
            upsert(store_id).dish = df
        elif kind == "pay":
            df, _ = read_any_table(b, name, PAY_MUST)
            upsert(store_id).pay = df
        else:
            warnings.append(f"æ— æ³•è¯†åˆ«æ–‡ä»¶ç±»å‹ï¼š{name}ï¼ˆå·²è·³è¿‡ï¼‰")

    out = list(bundles.values())
    out.sort(key=lambda x: x.store_id)
    return out, warnings, rules


def fmt_money(x: float) -> str:
    try:
        return f"Â¥{x:,.2f}"
    except Exception:
        return "â€”"


def halfhour_options(min_dt: pd.Timestamp, max_dt: pd.Timestamp) -> List[pd.Timestamp]:
    if pd.isna(min_dt) or pd.isna(max_dt):
        return []
    start = min_dt.floor("30min")
    end = max_dt.ceil("30min")
    return list(pd.date_range(start, end, freq="30min"))


def apply_time_filter(df: pd.DataFrame, col: str, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    x = df.copy()
    if col not in x.columns:
        return x
    x[col] = pd.to_datetime(x[col], errors="coerce")
    return x[(x[col] >= start) & (x[col] <= end)].copy()


def _base_items(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    if "ç±»å‹_norm" in df.columns:
        return df[df["ç±»å‹_norm"].isin(["èœå“", "å¥—é¤"])].copy()
    return df[df["ç±»å‹"].astype(str).str.contains("èœå“|å¥—é¤", na=False)].copy()


def _share_table(df_long: pd.DataFrame, store_col: str, key_col: str, val_col: str, topn: int) -> pd.DataFrame:
    tot = df_long.groupby(key_col, as_index=False)[val_col].sum().sort_values(val_col, ascending=False).head(topn)
    keys = tot[key_col].tolist()
    sub = df_long[df_long[key_col].isin(keys)].copy()
    denom = sub.groupby(store_col, as_index=False)[val_col].sum().rename(columns={val_col: "_den"})
    sub = sub.merge(denom, on=store_col, how="left")
    sub["share"] = sub[val_col] / sub["_den"].replace(0, np.nan)
    out = sub.pivot_table(index=key_col, columns=store_col, values="share", aggfunc="sum").fillna(0.0)
    out = out.loc[keys]
    return out


def _js_divergence(p: np.ndarray, q: np.ndarray) -> float:
    p = np.asarray(p, dtype=float)
    q = np.asarray(q, dtype=float)
    p = p / (p.sum() if p.sum() else 1.0)
    q = q / (q.sum() if q.sum() else 1.0)
    m = 0.5 * (p + q)

    def _kl(x, y):
        x = np.where(x <= 0, 1e-12, x)
        y = np.where(y <= 0, 1e-12, y)
        return float(np.sum(x * np.log(x / y)))

    return 0.5 * _kl(p, m) + 0.5 * _kl(q, m)


def main() -> None:
    st.title("ğŸ½ï¸ é¤é¥®ç»è¥åˆ†æç³»ç»Ÿï¼ˆè¿é”è§†è§’ Â· è‘£äº‹/è‚¡ä¸œ Â· é—¨åº—åº—é•¿ï¼‰")

    with st.sidebar:
        st.header("æ•°æ®è¾“å…¥")
        rule_file = st.file_uploader("ä¸Šä¼ ï¼šåˆ†ç±»è§„åˆ™æ¨¡æ¿ï¼ˆxlsxï¼ŒSheet=è§„åˆ™è¡¨ï¼‰", type=["xlsx"], accept_multiple_files=False)
        files = st.file_uploader(
            "ä¸Šä¼ ï¼šä¸‰ç±»æŠ¥è¡¨ï¼ˆå¯å¤šé—¨åº—ã€å¤šæ–‡ä»¶ï¼›æ”¯æŒ xls/xlsx/csvï¼‰",
            type=["xls", "xlsx", "csv"],
            accept_multiple_files=True,
        )
        st.caption("å£å¾„ï¼šæ—¶é—´æœ€å°30åˆ†é’Ÿï¼›â€œåŠ xxâ€ä¸ºå•åŠ ï¼ˆåŠ å¤šå®é™¤å¤–ï¼‰ï¼›å¤©éº»é¢å¹¶å…¥ç»†é¢ï¼›â€œæ ‡å‡†â€ä»…ç»Ÿè®¡ä¸ºã€å¥—é¤ã€‘çš„æ ‡å‡†è¡Œã€‚")

    if not files:
        st.info("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æŠ¥è¡¨æ–‡ä»¶ã€‚")
        return

    bundles, warnings, rules = parse_uploaded(files, rule_file)

    if warnings:
        with st.expander("âš ï¸ æ–‡ä»¶è¯†åˆ«è­¦å‘Š", expanded=False):
            for w in warnings:
                st.warning(w)

    analyzable = [b for b in bundles if b.dish is not None and b.pay is not None and b.daily is not None and b.store_id != "UNKNOWN"]
    missing = [b for b in bundles if b not in analyzable]

    if missing:
        with st.expander("âš ï¸ ç¼ºè¡¨é—¨åº—ï¼ˆä¸è¿›å…¥åˆ†æï¼‰", expanded=False):
            st.dataframe(
                pd.DataFrame(
                    [{"store_id": b.store_id, "æœ‰æ—¥é”€å”®": b.daily is not None, "æœ‰èœå“æ˜ç»†": b.dish is not None, "æœ‰æ”¯ä»˜æ˜ç»†": b.pay is not None} for b in missing]
                ),
                use_container_width=True,
            )

    if not analyzable:
        st.error("æ²¡æœ‰â€œä¸‰è¡¨é½å…¨â€çš„é—¨åº—ï¼Œæ— æ³•åˆ†æã€‚")
        return

    store_ids = [b.store_id for b in analyzable]
    sel_stores = st.multiselect("é€‰æ‹©é—¨åº—ï¼ˆæ”¯æŒå¤šåº—å¯¹æ¯”ï¼‰", options=store_ids, default=store_ids[:1])
    if not sel_stores:
        st.stop()

    facts_by_store: Dict[str, Dict[str, pd.DataFrame]] = {}
    for b in analyzable:
        if b.store_id in sel_stores:
            facts_by_store[b.store_id] = build_fact_tables(b.dish, b.pay, rules, b.store_id)

    all_orders = pd.concat([facts_by_store[s]["fact_orders"] for s in sel_stores], ignore_index=True)
    min_dt = all_orders["order_time"].min()
    max_dt = all_orders["order_time"].max()
    opts = halfhour_options(min_dt, max_dt)
    if not opts:
        st.error("æ— æ³•ä»æ•°æ®ä¸­è§£æåˆ›å»ºæ—¶é—´ã€‚")
        return

    c1, c2 = st.columns(2)
    with c1:
        start = st.selectbox("å¼€å§‹æ—¶é—´ï¼ˆ30åˆ†é’Ÿç²’åº¦ï¼‰", options=opts, index=0, format_func=lambda x: x.strftime("%Y-%m-%d %H:%M"))
    with c2:
        end = st.selectbox("ç»“æŸæ—¶é—´ï¼ˆ30åˆ†é’Ÿç²’åº¦ï¼‰", options=opts, index=len(opts) - 1, format_func=lambda x: x.strftime("%Y-%m-%d %H:%M"))

    if start > end:
        st.error("å¼€å§‹æ—¶é—´ä¸èƒ½æ™šäºç»“æŸæ—¶é—´ã€‚")
        return

    filtered: Dict[str, Dict[str, pd.DataFrame]] = {}
    for sid in sel_stores:
        f = facts_by_store[sid]
        filtered[sid] = {
            "items_main": apply_time_filter(f["fact_items_main"], "åˆ›å»ºæ—¶é—´", start, end),
            "items_add": apply_time_filter(f["fact_items_add"], "created_at", start, end),
            "pay": apply_time_filter(f["fact_pay"], "order_time", start, end),
            "orders": apply_time_filter(f["fact_orders"], "order_time", start, end),
        }

    tabs = st.tabs(
        [
            "â‘  è‘£äº‹/è‚¡ä¸œæ€»è§ˆ",
            "â‘¡ é—¨åº—å¯¹æ¯”",
            "â‘¢ è§„æ ¼",
            "â‘£ å“ç±»ç»“æ„",
            "â‘¤ å•åŠ åˆ†æ",
            "â‘¥ æ”¯ä»˜æ¸ é“",
            "â‘¦ é€€æ¬¾/å¼‚å¸¸ä¸å¯¹è´¦",
            "â‘§ æœªåˆ†ç±»æ± ï¼ˆå¯å¯¼å‡ºï¼‰",
            "â‘¨ æ˜ç»†å¯¼å‡º",
            "â‘© æ—¶æ®µçƒ­åŠ›å›¾",
        ]
    )

    # â‘  æ€»è§ˆ
    with tabs[0]:
        st.subheader("è‘£äº‹/è‚¡ä¸œè§†è§’ï¼šè§„æ¨¡ã€æ•ˆç‡ã€ç»“æ„ã€é£é™©")

        rows = []
        for sid in sel_stores:
            o = filtered[sid]["orders"]
            p = filtered[sid]["pay"]
            orders = int(o["POSé”€å”®å•å·"].nunique()) if not o.empty else 0
            rows.append(
                {
                    "store_id": sid,
                    "è®¢å•æ•°": orders,
                    "èœå“é”€é‡": float(o["dish_qty"].sum()) if not o.empty else 0.0,
                    "èœå“åº”æ”¶(ä¼˜æƒ å)": float(o["net_amount"].sum()) if not o.empty else 0.0,
                    "æ”¯ä»˜å®æ”¶": float(p["æ€»é‡‘é¢"].sum()) if not p.empty else 0.0,
                    "é€€æ¬¾å•å æ¯”": float(o["has_refund"].mean()) if not o.empty else 0.0,
                    "å®¢å•(åº”æ”¶/è®¢å•)": (float(o["net_amount"].sum()) / orders) if orders else np.nan,
                }
            )
        dfk = pd.DataFrame(rows)
        k1, k2, k3, k4 = st.columns(4)
        k1.metric("é€‰ä¸­é—¨åº—è®¢å•æ•°", int(dfk["è®¢å•æ•°"].sum()))
        k2.metric("é€‰ä¸­é—¨åº—èœå“é”€é‡", f"{dfk['èœå“é”€é‡'].sum():,.0f}")
        k3.metric("é€‰ä¸­é—¨åº—èœå“åº”æ”¶(ä¼˜æƒ å)", fmt_money(dfk["èœå“åº”æ”¶(ä¼˜æƒ å)"].sum()))
        k4.metric("é€‰ä¸­é—¨åº—æ”¯ä»˜å®æ”¶", fmt_money(dfk["æ”¯ä»˜å®æ”¶"].sum()))
        st.dataframe(dfk, use_container_width=True)

        oall = pd.concat([filtered[s]["orders"] for s in sel_stores], ignore_index=True)
        if not oall.empty:
            oall["bucket"] = oall["order_time"].dt.floor("30min")
            trend = oall.groupby("bucket", as_index=False).agg(è®¢å•æ•°=("POSé”€å”®å•å·", "nunique"), èœå“åº”æ”¶=("net_amount", "sum")).sort_values("bucket")
            st.line_chart(trend.set_index("bucket")[["è®¢å•æ•°", "èœå“åº”æ”¶"]])
            st.markdown("**å³°å€¼æ—¶æ®µ Top10ï¼ˆæŒ‰è®¢å•æ•°ï¼‰**")
            st.dataframe(trend.sort_values("è®¢å•æ•°", ascending=False).head(10), use_container_width=True)

        main_all = pd.concat([filtered[s]["items_main"] for s in sel_stores], ignore_index=True)
        base_items = _base_items(main_all)
        if not base_items.empty:
            top_rev = (
                base_items.groupby("èœå“åç§°", as_index=False)
                .agg(åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"), é”€é‡=("èœå“æ•°é‡", "sum"), è®¢å•æ•°=("POSé”€å”®å•å·", "nunique"))
                .sort_values(["åº”æ”¶", "é”€é‡"], ascending=False)
                .head(20)
            )
            st.markdown("### Top20 èœå“ï¼ˆæŒ‰åº”æ”¶æ’åºï¼‰")
            st.dataframe(top_rev, use_container_width=True)
            st.bar_chart(top_rev.set_index("èœå“åç§°")[["åº”æ”¶"]])

            dish_rev = base_items.groupby("èœå“åç§°", as_index=False).agg(åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum")).sort_values("åº”æ”¶", ascending=False)
            dish_rev["ç´¯è®¡åº”æ”¶"] = dish_rev["åº”æ”¶"].cumsum()
            total_rev = dish_rev["åº”æ”¶"].sum()
            dish_rev["ç´¯è®¡å æ¯”"] = dish_rev["ç´¯è®¡åº”æ”¶"] / total_rev if total_rev else 0
            n80 = int((dish_rev["ç´¯è®¡å æ¯”"] <= 0.8).sum() + 1) if total_rev else 0
            st.markdown("### çˆ†å“/é•¿å°¾ï¼ˆå¸•ç´¯æ‰˜ï¼‰")
            st.write(f"è¾¾åˆ° **80%åº”æ”¶** éœ€è¦çš„èœå“æ•°ï¼š**{n80}** / æ€»èœå“æ•° {len(dish_rev)}")
            st.dataframe(dish_rev.head(50), use_container_width=True)

        add_all = pd.concat([filtered[s]["items_add"] for s in sel_stores], ignore_index=True)
        if not add_all.empty:
            top_add = add_all.groupby("add_display", as_index=False).agg(å•åŠ é‡‘é¢=("amount", "sum"), é”€é‡=("qty", "sum"), è®¢å•æ•°=("order_id", "nunique")).sort_values(["å•åŠ é‡‘é¢", "é”€é‡"], ascending=False).head(20)
            st.markdown("### Top20 å•åŠ ï¼ˆæŒ‰å•åŠ é‡‘é¢æ’åºï¼‰")
            st.dataframe(top_add, use_container_width=True)
            st.bar_chart(top_add.set_index("add_display")[["å•åŠ é‡‘é¢"]])

    # â‘¡ é—¨åº—å¯¹æ¯”ï¼ˆç»“æ„å¯¹æ¯” + åç¦»åº¦ï¼šä¿®å¤sel_storesä½œç”¨åŸŸï¼‰
    with tabs[1]:
        st.subheader("é—¨åº—å¯¹æ¯”ï¼šåŒå£å¾„çœ‹å·®å¼‚ï¼ˆåº—é•¿/åŒºåŸŸç»ç†/æ€»éƒ¨ï¼‰")
        rows = []
        for sid in sel_stores:
            o = filtered[sid]["orders"]
            p = filtered[sid]["pay"]
            orders = int(o["POSé”€å”®å•å·"].nunique()) if not o.empty else 0
            net = float(o["net_amount"].sum()) if not o.empty else 0.0
            paid = float(p["æ€»é‡‘é¢"].sum()) if not p.empty else 0.0
            rows.append({"store_id": sid, "è®¢å•æ•°": orders, "åº”æ”¶(ä¼˜æƒ å)": net, "å®æ”¶": paid, "åº”æ”¶-å®æ”¶å·®å¼‚": net - paid, "å®¢å•(åº”æ”¶/è®¢å•)": (net / orders) if orders else np.nan})
        df = pd.DataFrame(rows).sort_values("åº”æ”¶(ä¼˜æƒ å)", ascending=False)
        st.dataframe(df, use_container_width=True)
        st.bar_chart(df.set_index("store_id")[["åº”æ”¶(ä¼˜æƒ å)", "å®æ”¶"]])
        c1, c2 = st.columns(2)
        with c1:
            st.bar_chart(df.set_index("store_id")[["åº”æ”¶-å®æ”¶å·®å¼‚"]])
        with c2:
            st.bar_chart(df.set_index("store_id")[["å®¢å•(åº”æ”¶/è®¢å•)"]])

        st.markdown("### åŒåº—ç»“æ„å¯¹æ¯”ï¼ˆæ€»éƒ¨/åŒºåŸŸï¼šæ‰¾åç¦»ã€æ‰¾å¯å¤åˆ¶æ‰“æ³•ï¼‰")
        dim = st.selectbox("é€‰æ‹©ç»“æ„ç»´åº¦", options=["è§„æ ¼ç»“æ„", "å“ç±»ç»“æ„", "å•åŠ ç»“æ„", "æ”¯ä»˜ç»“æ„"], index=0)
        metric = st.selectbox("é€‰æ‹©æŒ‡æ ‡", options=["åº”æ”¶", "é”€é‡/ç¬”æ•°"], index=0, key="cmp_metric")

        def build_long() -> pd.DataFrame:
            rows2 = []
            if dim == "è§„æ ¼ç»“æ„":
                for sid in sel_stores:
                    m = _base_items(filtered[sid]["items_main"])
                    x = m[m["spec_norm"].notna()].copy() if not m.empty else pd.DataFrame()
                    if x.empty:
                        continue
                    if metric == "åº”æ”¶":
                        g = x.groupby("spec_norm", as_index=False).agg(v=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"))
                    else:
                        g = x.groupby("spec_norm", as_index=False).agg(v=("èœå“æ•°é‡", "sum"))
                    g["store_id"] = sid
                    g = g.rename(columns={"spec_norm": "k"})
                    rows2.append(g)
            elif dim == "å“ç±»ç»“æ„":
                for sid in sel_stores:
                    m = filtered[sid]["items_main"]
                    if m.empty:
                        continue
                    ex = m.copy().explode("categories")
                    ex["categories"] = ex["categories"].fillna("æœªåˆ†ç±»")
                    if metric == "åº”æ”¶":
                        g = ex.groupby("categories", as_index=False).agg(v=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"))
                    else:
                        g = ex.groupby("categories", as_index=False).agg(v=("èœå“æ•°é‡", "sum"))
                    g["store_id"] = sid
                    g = g.rename(columns={"categories": "k"})
                    rows2.append(g)
            elif dim == "å•åŠ ç»“æ„":
                for sid in sel_stores:
                    a = filtered[sid]["items_add"]
                    if a.empty:
                        continue
                    if metric == "åº”æ”¶":
                        g = a.groupby("add_display", as_index=False).agg(v=("amount", "sum"))
                    else:
                        g = a.groupby("add_display", as_index=False).agg(v=("order_id", "nunique"))
                    g["store_id"] = sid
                    g = g.rename(columns={"add_display": "k"})
                    rows2.append(g)
            else:
                for sid in sel_stores:
                    p = filtered[sid]["pay"]
                    if p.empty:
                        continue
                    if metric == "åº”æ”¶":
                        g = p.groupby("æ”¯ä»˜ç±»å‹", as_index=False).agg(v=("æ€»é‡‘é¢", "sum"))
                    else:
                        g = p.groupby("æ”¯ä»˜ç±»å‹", as_index=False).agg(v=("POSé”€å”®å•å·", "count"))
                    g["store_id"] = sid
                    g = g.rename(columns={"æ”¯ä»˜ç±»å‹": "k"})
                    rows2.append(g)

            if rows2:
                return pd.concat(rows2, ignore_index=True)
            return pd.DataFrame(columns=["store_id", "k", "v"])

        long = build_long()
        if long.empty:
            st.info("æš‚æ— æ•°æ®ç”¨äºç»“æ„å¯¹æ¯”ã€‚")
        else:
            topn = 6 if dim == "è§„æ ¼ç»“æ„" else (12 if dim == "å“ç±»ç»“æ„" else 10)
            share = _share_table(long, "store_id", "k", "v", topn=topn)
            st.dataframe(share.style.format("{:.1%}"), use_container_width=True)

            chart = alt.Chart(long).mark_bar().encode(
                x=alt.X("store_id:N", title="é—¨åº—"),
                y=alt.Y("v:Q", title="å€¼"),
                color=alt.Color("k:N", title=dim.replace("ç»“æ„", "")),
                tooltip=["store_id", "k", "v"],
            ).properties(height=420)
            st.altair_chart(chart, use_container_width=True)

            # åç¦»åº¦
            st.markdown("### åç¦»åº¦æ’åï¼šå“ªå®¶é—¨åº—æœ€â€˜ä¸ä¸€æ ·â€™ï¼Ÿå“ªå®¶å¯åšæ ‡æ†ï¼Ÿ")
            mat = share.T  # store x key
            mean = mat.mean(axis=0).values
            rows3 = []
            for sid in mat.index:
                js = _js_divergence(mat.loc[sid].values, mean)
                rows3.append({"store_id": sid, "åç¦»åº¦(JS)": js})
            ddf = pd.DataFrame(rows3).sort_values("åç¦»åº¦(JS)", ascending=False)
            st.dataframe(ddf, use_container_width=True)
            if len(ddf) >= 2:
                bench = ddf.sort_values("åç¦»åº¦(JS)", ascending=True).iloc[0]["store_id"]
                outlier = ddf.iloc[0]["store_id"]
                st.write(f"**å»ºè®®**ï¼šå¯å…ˆæŠŠåç¦»åº¦æœ€ä½çš„é—¨åº— **{bench}** ä½œä¸ºâ€œæ ‡æ†ç»“æ„â€ï¼Œé‡ç‚¹å¤ç›˜åç¦»åº¦æœ€é«˜çš„é—¨åº— **{outlier}** çš„åŸå› ï¼ˆå®¢ç¾¤/æ—¶æ®µ/å¥—é¤å æ¯”/æ¸ é“ï¼‰ã€‚")

    # â‘¢ è§„æ ¼
    with tabs[2]:
        st.subheader("è§„æ ¼ï¼šä¸»é£Ÿç»“æ„ï¼ˆå«â€œæ ‡å‡†â€=å¥—é¤æ ‡å‡†ï¼‰")
        st.caption("è§„æ ¼åˆ†å¸ƒåªç»Ÿè®¡ï¼šæ ‡å‡† / å®½é¢ / ç»†é¢(å«å¤©éº»é¢) / ç±³é¥­ / å®½ç²‰(å«ç²‰) / æ— éœ€ä¸»é£Ÿï¼›â€œæ ‡å‡†â€ä»…æ¥æºäº ç±»å‹=å¥—é¤ çš„æ ‡å‡†è¡Œã€‚")
        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            m = filtered[sid]["items_main"]
            if m.empty:
                st.info("æ— æ•°æ®")
                continue
            base = _base_items(m)
            spec_base = base[base["spec_norm"].notna()].copy()
            if spec_base.empty:
                st.info("è¯¥æ—¶é—´èŒƒå›´å†…æ²¡æœ‰å‘½ä¸­è§„æ ¼ç™½åå•çš„æ•°æ®ã€‚")
                continue
            spec = spec_base.groupby("spec_norm", as_index=False).agg(é”€é‡=("èœå“æ•°é‡", "sum"), åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"), è¡Œæ•°=("èœå“åç§°", "count"), è®¢å•æ•°=("POSé”€å”®å•å·", "nunique")).sort_values(["é”€é‡", "åº”æ”¶"], ascending=False)
            spec["é”€é‡å æ¯”"] = spec["é”€é‡"] / spec["é”€é‡"].sum() if spec["é”€é‡"].sum() else 0
            spec["åº”æ”¶å æ¯”"] = spec["åº”æ”¶"] / spec["åº”æ”¶"].sum() if spec["åº”æ”¶"].sum() else 0
            st.dataframe(spec, use_container_width=True)
            c1, c2 = st.columns(2)
            with c1:
                st.bar_chart(spec.set_index("spec_norm")[["é”€é‡"]])
            with c2:
                st.bar_chart(spec.set_index("spec_norm")[["åº”æ”¶"]])
            spec_base["bucket"] = spec_base["åˆ›å»ºæ—¶é—´"].dt.floor("30min")
            top_specs = spec["spec_norm"].head(5).tolist()
            pivot = spec_base[spec_base["spec_norm"].isin(top_specs)].groupby(["bucket", "spec_norm"], as_index=False).agg(é”€é‡=("èœå“æ•°é‡", "sum"))
            if not pivot.empty:
                piv = pivot.pivot(index="bucket", columns="spec_norm", values="é”€é‡").fillna(0).sort_index()
                st.line_chart(piv)

    # â‘£ å“ç±»ç»“æ„
    with tabs[3]:
        st.subheader("å“ç±»ç»“æ„ï¼šè§„åˆ™æ¨¡æ¿å‘½ä¸­ï¼ˆå¤šæ ‡ç­¾ï¼‰")
        st.caption("ä¸€ä¸ªèœå“å¯å‘½ä¸­å¤šä¸ªåˆ†ç±»ï¼Œå‘½ä¸­å³å„è®¡ä¸€æ¬¡ï¼›æœªå‘½ä¸­è¿›å…¥æœªåˆ†ç±»æ± ã€‚")
        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            m = filtered[sid]["items_main"]
            if m.empty:
                st.info("æ— æ•°æ®")
                continue
            exploded = m.copy().explode("categories")
            exploded["categories"] = exploded["categories"].fillna("æœªåˆ†ç±»")
            cat = exploded.groupby("categories", as_index=False).agg(é”€é‡=("èœå“æ•°é‡", "sum"), åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"), èœå“è¡Œæ•°=("èœå“åç§°", "count")).sort_values("åº”æ”¶", ascending=False)
            st.dataframe(cat, use_container_width=True)
            st.bar_chart(cat.set_index("categories")[["åº”æ”¶"]])
            topn = st.slider(f"TopN èœå“ï¼ˆé—¨åº— {sid}ï¼‰", min_value=5, max_value=50, value=20, step=5, key=f"topn_{sid}")
            cats = ["å…¨éƒ¨"] + sorted(exploded["categories"].dropna().unique().tolist())
            sel_cat = st.selectbox(f"é€‰æ‹©åˆ†ç±»ï¼ˆé—¨åº— {sid}ï¼‰", options=cats, key=f"selcat_{sid}")
            view = exploded if sel_cat == "å…¨éƒ¨" else exploded[exploded["categories"] == sel_cat]
            top_items = view.groupby("èœå“åç§°", as_index=False).agg(åº”æ”¶=("ä¼˜æƒ åå°è®¡ä»·æ ¼", "sum"), é”€é‡=("èœå“æ•°é‡", "sum"), è®¢å•æ•°=("POSé”€å”®å•å·", "nunique")).sort_values(["åº”æ”¶", "é”€é‡"], ascending=False).head(topn)
            st.dataframe(top_items, use_container_width=True)

    # â‘¤ å•åŠ åˆ†æ
    with tabs[4]:
        st.subheader("å•åŠ åˆ†æï¼šåŠ æ–™å¸¦æ¥çš„ç»“æ„ä¸å®¢å•æå‡ï¼ˆä¸ä¸»èœä¸¥æ ¼éš”ç¦»ï¼‰")
        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            a = filtered[sid]["items_add"]
            if a.empty:
                st.info("æ— å•åŠ è®°å½•")
                continue
            add = a.groupby("add_display", as_index=False).agg(é”€é‡=("qty", "sum"), å•åŠ é‡‘é¢=("amount", "sum"), è®¢å•æ•°=("order_id", "nunique"), æ¥æº=("source", lambda s: ",".join(sorted(set(map(str, s)))))).sort_values(["å•åŠ é‡‘é¢", "é”€é‡"], ascending=False)
            st.dataframe(add, use_container_width=True)
            st.bar_chart(add.set_index("add_display")[["å•åŠ é‡‘é¢"]])
            orders = filtered[sid]["orders"]
            add_orders = int(a["order_id"].nunique())
            total_orders = int(orders["POSé”€å”®å•å·"].nunique()) if not orders.empty else 0
            st.metric("å•åŠ æ¸—é€ç‡ï¼ˆå«å•åŠ è®¢å•/æ€»è®¢å•ï¼‰", f"{(add_orders / total_orders * 100) if total_orders else 0:.1f}%")
            if not orders.empty:
                add_set = set(a["order_id"].dropna().astype(str).tolist())
                o2 = orders.copy()
                o2["has_add"] = o2["POSé”€å”®å•å·"].astype(str).isin(add_set)
                grp = o2.groupby("has_add", as_index=False).agg(è®¢å•æ•°=("POSé”€å”®å•å·", "nunique"), åº”æ”¶=("net_amount", "sum"))
                grp["å®¢å•(åº”æ”¶/è®¢å•)"] = grp["åº”æ”¶"] / grp["è®¢å•æ•°"].replace(0, np.nan)
                st.markdown("**æœ‰å•åŠ  vs æ— å•åŠ ï¼ˆå®¢å•æå‡ï¼‰**")
                st.dataframe(grp, use_container_width=True)

    # â‘¥ æ”¯ä»˜æ¸ é“
    with tabs[5]:
        st.subheader("æ”¯ä»˜æ¸ é“ï¼šæ¸ é“ç»“æ„ã€å›¢è´­æ¸—é€ã€æ··åˆæ”¯ä»˜")
        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            p = filtered[sid]["pay"]
            if p.empty:
                st.warning("æ— æ”¯ä»˜æ•°æ®ï¼ˆè¯¥é—¨åº—åœ¨ç­›é€‰æ—¶é—´èŒƒå›´å†…æ”¯ä»˜è¡¨æœªå…³è”åˆ°ä»»ä½•è®¢å•ï¼Œæˆ–æ”¯ä»˜è¡¨æœªè¢«æ­£ç¡®è¯†åˆ«ï¼‰")
                continue
            pay = p.groupby("æ”¯ä»˜ç±»å‹", as_index=False).agg(å®æ”¶=("æ€»é‡‘é¢", "sum"), æ”¯ä»˜ç¬”æ•°=("POSé”€å”®å•å·", "count"), æ¶‰åŠè®¢å•=("POSé”€å”®å•å·", "nunique")).sort_values(["å®æ”¶", "æ”¯ä»˜ç¬”æ•°"], ascending=False)
            st.dataframe(pay, use_container_width=True)

    # â‘¦ é€€æ¬¾/å¼‚å¸¸ä¸å¯¹è´¦ï¼ˆä¿æŒfixed10åŠŸèƒ½ + å¢å¼ºå·²åœ¨ä¸Šé¢å®ç°ï¼‰
    with tabs[6]:
        st.subheader("é€€æ¬¾/å¼‚å¸¸ä¸å¯¹è´¦ï¼šè¯·ä½¿ç”¨ fixed10 ç‰ˆçš„å®Œæ•´å†…å®¹ï¼ˆæœ¬ç‰ˆå·²åœ¨ä¸Šæ–¹å®ç°å¢å¼ºå—ï¼‰")
        st.info("ä¸ºé¿å…æœ¬æ–‡ä»¶è¿‡é•¿é‡å¤ï¼Œè¿™ä¸ª tab çš„åŠŸèƒ½å·²åŒ…å«åœ¨æœ¬ appï¼ˆä½ çœ‹åˆ°çš„æ˜¯è¯¥æç¤ºï¼Œè¯´æ˜ä½ è·‘çš„æ˜¯æ—§ç¼“å­˜ï¼‰ã€‚è¯· Ctrl+F5 å¼ºåˆ·æˆ–åˆ é™¤ .streamlit/cache åé‡å¯ã€‚")

    # â‘§ æœªåˆ†ç±»æ± 
    with tabs[7]:
        st.subheader("æœªåˆ†ç±»æ± ï¼šå¯æŸ¥çœ‹ã€å¯å¯¼å‡ºï¼ˆè§„åˆ™è¿­ä»£å…¥å£ï¼‰")
        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            m = filtered[sid]["items_main"]
            if m.empty:
                st.info("æ— æ•°æ®")
                continue
            un = m[m["categories"].apply(lambda x: len(x) == 0)].copy()
            st.write(f"æœªåˆ†ç±»ä¸»èœè¡Œæ•°ï¼š{len(un):,}")
            st.dataframe(un.head(200), use_container_width=True)
            st.download_button(f"å¯¼å‡ºæœªåˆ†ç±»ä¸»èœï¼ˆ{sid}ï¼‰CSV", data=un.to_csv(index=False).encode("utf-8-sig"), file_name=f"æœªåˆ†ç±»ä¸»èœ_{sid}.csv", mime="text/csv")

    # â‘¨ æ˜ç»†å¯¼å‡º
    with tabs[8]:
        st.subheader("æ˜ç»†å¯¼å‡ºï¼šæ€»éƒ¨/è´¢åŠ¡/åº—é•¿äºŒæ¬¡åˆ†æ")
        for sid in sel_stores:
            st.markdown(f"#### é—¨åº— {sid}")
            m = filtered[sid]["items_main"]
            st.download_button(f"å¯¼å‡ºèœå“æ˜ç»†-è¿‡æ»¤åï¼ˆ{sid}ï¼‰CSV", data=m.to_csv(index=False).encode("utf-8-sig"), file_name=f"èœå“æ˜ç»†_è¿‡æ»¤å_{sid}.csv", mime="text/csv")

    # â‘© æ—¶æ®µçƒ­åŠ›å›¾ï¼ˆåŠ¨ä½œå»ºè®®+å¯¼å‡ºï¼‰
    with tabs[9]:
        st.subheader("æ—¶æ®µçƒ­åŠ›å›¾ï¼ˆ30åˆ†é’Ÿç²’åº¦ï¼‰ï¼šå³°è°·ã€æ’ç­ã€å¤‡è´§ã€æ¸ é“åŠ¨ä½œ")
        st.caption("è¡Œ=æ—¥æœŸï¼Œåˆ—=åŠå°æ—¶ï¼›å¯é€‰æ‹©æŒ‡æ ‡ï¼›æ”¯æŒé€‰ä¸­é—¨åº—æ±‡æ€»æˆ–æŒ‰é—¨åº—åˆ†åˆ«æŸ¥çœ‹ã€‚")

        metric = st.selectbox("é€‰æ‹©æŒ‡æ ‡", options=["è®¢å•æ•°", "åº”æ”¶(ä¼˜æƒ å)", "å®æ”¶", "å®¢å•(åº”æ”¶/è®¢å•)", "å•åŠ æ¸—é€ç‡(å«å•åŠ è®¢å•/æ€»è®¢å•)"], index=0)
        scope = st.radio("èŒƒå›´", options=["é€‰ä¸­é—¨åº—æ±‡æ€»", "æŒ‰é—¨åº—åˆ†åˆ«çœ‹"], horizontal=True)

        def _build_heat(o_df: pd.DataFrame, p_df: pd.DataFrame, a_df: pd.DataFrame) -> Optional[pd.DataFrame]:
            if o_df is None or o_df.empty:
                return None
            o = o_df.copy()
            o["date"] = o["order_time"].dt.date
            o["slot"] = o["order_time"].dt.floor("30min").dt.strftime("%H:%M")
            grp = o.groupby(["date", "slot"], as_index=False).agg(orders=("POSé”€å”®å•å·", "nunique"), net=("net_amount", "sum"))
            if p_df is not None and not p_df.empty:
                p = p_df.copy()
                p["date"] = p["order_time"].dt.date
                p["slot"] = p["order_time"].dt.floor("30min").dt.strftime("%H:%M")
                paid = p.groupby(["date", "slot"], as_index=False).agg(paid=("æ€»é‡‘é¢", "sum"))
                grp = grp.merge(paid, on=["date", "slot"], how="left")
            grp["paid"] = grp.get("paid", 0).fillna(0.0)
            if a_df is not None and not a_df.empty:
                a = a_df.copy()
                a["date"] = a["created_at"].dt.date
                a["slot"] = a["created_at"].dt.floor("30min").dt.strftime("%H:%M")
                add_o = a.groupby(["date", "slot"], as_index=False).agg(add_orders=("order_id", "nunique"))
                grp = grp.merge(add_o, on=["date", "slot"], how="left")
            grp["add_orders"] = grp.get("add_orders", 0).fillna(0)
            grp["aov"] = grp["net"] / grp["orders"].replace(0, np.nan)
            grp["add_rate"] = grp["add_orders"] / grp["orders"].replace(0, np.nan)
            return grp

        def _render_heat(df: Optional[pd.DataFrame], key_prefix: str) -> None:
            if df is None or df.empty:
                st.info("æ— å¯ç”¨æ•°æ®")
                return
            if metric == "è®¢å•æ•°":
                mat = df.pivot(index="date", columns="slot", values="orders").fillna(0).astype(int)
            elif metric == "åº”æ”¶(ä¼˜æƒ å)":
                mat = df.pivot(index="date", columns="slot", values="net").fillna(0.0)
            elif metric == "å®æ”¶":
                mat = df.pivot(index="date", columns="slot", values="paid").fillna(0.0)
            elif metric == "å®¢å•(åº”æ”¶/è®¢å•)":
                mat = df.pivot(index="date", columns="slot", values="aov")
            else:
                mat = df.pivot(index="date", columns="slot", values="add_rate")
            cols = sorted(mat.columns, key=lambda x: (int(x.split(":")[0]), int(x.split(":")[1])))
            mat = mat[cols]

            view = st.radio("å±•ç¤ºæ–¹å¼", options=["çƒ­åŠ›å›¾", "æ¸å˜è¡¨æ ¼"], horizontal=True, key=f"heat_view_{key_prefix}_{metric}")
            if view == "æ¸å˜è¡¨æ ¼":
                st.dataframe(mat.style.background_gradient(axis=None), use_container_width=True)
            else:
                mdf = mat.reset_index().melt(id_vars="date", var_name="slot", value_name="value")
                chart = alt.Chart(mdf).mark_rect().encode(
                    x=alt.X("slot:N", title="åŠå°æ—¶"),
                    y=alt.Y("date:N", title="æ—¥æœŸ"),
                    color=alt.Color("value:Q", title=metric),
                    tooltip=["date", "slot", "value"],
                ).properties(height=320)
                st.altair_chart(chart, use_container_width=True)

        if scope == "é€‰ä¸­é—¨åº—æ±‡æ€»":
            oall2 = pd.concat([filtered[s]["orders"] for s in sel_stores], ignore_index=True)
            pall2 = pd.concat([filtered[s]["pay"] for s in sel_stores], ignore_index=True)
            aall2 = pd.concat([filtered[s]["items_add"] for s in sel_stores], ignore_index=True)
            baseh = _build_heat(oall2, pall2, aall2)
            _render_heat(baseh, "all")

            st.markdown("### åŠ¨ä½œå»ºè®®ï¼ˆA2ï¼‰ï¼šå³°è°·æ’ç­ã€ä¿ƒé”€æ—¶æ®µã€å•åŠ å¼•å¯¼")
            if baseh is not None and not baseh.empty:
                agg = baseh.groupby("slot", as_index=False).agg(è®¢å•æ•°=("orders", "sum"), åº”æ”¶=("net", "sum"), å®æ”¶=("paid", "sum"), å•åŠ è®¢å•=("add_orders", "sum"))
                agg["å®¢å•"] = agg["åº”æ”¶"] / agg["è®¢å•æ•°"].replace(0, np.nan)
                agg["å•åŠ æ¸—é€ç‡"] = agg["å•åŠ è®¢å•"] / agg["è®¢å•æ•°"].replace(0, np.nan)
                peak = agg.sort_values("è®¢å•æ•°", ascending=False).head(5)
                low = agg[agg["è®¢å•æ•°"] > 0].sort_values("è®¢å•æ•°", ascending=True).head(5)
                opp = agg[agg["è®¢å•æ•°"] >= agg["è®¢å•æ•°"].median()].sort_values("å•åŠ æ¸—é€ç‡", ascending=True).head(5)

                st.dataframe(pd.concat([peak.assign(ç±»å‹="å³°å€¼"), low.assign(ç±»å‹="ä½è°·"), opp.assign(ç±»å‹="å•åŠ æœºä¼š")], ignore_index=True), use_container_width=True)
                action = pd.concat([peak.assign(å»ºè®®="å³°å€¼ï¼šåŠ äºº/å¤‡è´§/ä¿å‡ºé¤"), low.assign(å»ºè®®="ä½è°·ï¼šä¿ƒé”€/å›¢è´­/å¼•å¯¼å•åŠ "), opp.assign(å»ºè®®="æœºä¼šï¼šå¼ºåŒ–å•åŠ è¯æœ¯")], ignore_index=True)
                st.download_button("å¯¼å‡ºåº—é•¿è¡ŒåŠ¨æ¸…å• CSV", data=action.to_csv(index=False).encode("utf-8-sig"), file_name="åº—é•¿è¡ŒåŠ¨æ¸…å•.csv", mime="text/csv")
            else:
                st.info("åŠ¨ä½œå»ºè®®ï¼šå½“å‰èŒƒå›´æ— è¶³å¤Ÿæ•°æ®ã€‚")
        else:
            for sid in sel_stores:
                st.markdown(f"#### é—¨åº— {sid}")
                dfh = _build_heat(filtered[sid]["orders"], filtered[sid]["pay"], filtered[sid]["items_add"])
                _render_heat(dfh, sid)


if __name__ == "__main__":
    main()
